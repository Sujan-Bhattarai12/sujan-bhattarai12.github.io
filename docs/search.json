[
  {
    "objectID": "featured_projects.html",
    "href": "featured_projects.html",
    "title": "",
    "section": "",
    "text": "Using AI for Good: Helping non profits in predicting carbon calculation\n\n\n\n\n\n\nMachine Learning\n\n\nPython\n\n\n\n\n\n\n\n\n\nSujan Bhattarai\n\n\n\n\n\n\n\n\n\n\n\n\nDid Smoking Mothers Have Lowered Weight Children?\n\n\n\n\n\n\nStatistics\n\n\nEconometrics\n\n\nCausal Inference\n\n\nR\n\n\n\n\n\n\n\n\n\nSujan Bhattarai\n\n\n\n\n\n\n\n\n\n\n\n\nComparing competing Machine Learning models in classifying Spotify songs\n\n\n\n\n\n\nMachine Learning\n\n\n\n\n\n\n\n\n\nJan 20, 2024\n\n\nSujan Bhattarai\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html#updates",
    "href": "about.html#updates",
    "title": "\n\n",
    "section": "Updates:",
    "text": "Updates:\n\nüìä Practising SQL from HackerRank and learning DSA in LeetCode üß† Mastering neural network architecture and learning PyTorch from Coursera and DataCamp\n\n\nRecently completed:\n\nüñ• Worked on 4TB data on NCAR supercomputer, for developing Machine Learning model and a web-interactive dashboard üöÄ Learnt Big Data System with Apache Hive & Impala üìö Read Machine Learning with Scikit Learn book by O‚ÄôReilly üåê Initiated a new Python project for a new blog in ML"
  },
  {
    "objectID": "about.html#tech-skills",
    "href": "about.html#tech-skills",
    "title": "\n\n",
    "section": "Tech Skills",
    "text": "Tech Skills\n\nPython üêç\nR üìä\nSQL üíæ\nSAS üìà\nBig Data üåê\nMachine Learning ü§ñ\nData Visualization üìä\nGeospatial Analysis üåç\nData Mining ‚õèÔ∏è\nHigh performance computing üöÄ\nBash Scripting üìú\nWeb Development üåê"
  },
  {
    "objectID": "about.html#blogs",
    "href": "about.html#blogs",
    "title": "\n\n",
    "section": "Blogs:",
    "text": "Blogs:\n\n\nForecasting Air-Quality Emission üå¨Ô∏è\n\n\nGeospatial Analysis üó∫Ô∏è\n\n\nArtificial Intelligence and Nature Conservation üåø"
  },
  {
    "objectID": "about.html#statistics",
    "href": "about.html#statistics",
    "title": "\n\n",
    "section": "Statistics:",
    "text": "Statistics:\n üìà\n\nThanks for Visiting\nIf you have come this far, revise Hadoop CLI(HDFS DFS) commands, extremely useful if you are using SQL for Big data analysis) Apache_CLI"
  },
  {
    "objectID": "featured_projects/ml_spotify/index.html",
    "href": "featured_projects/ml_spotify/index.html",
    "title": "Comparing competing Machine Learning models in classifying Spotify songs",
    "section": "",
    "text": "In this project, I explored four popular machine learning models (K-Nearest Neighbors, Decision Tree, Bagged Tree, and Random Forest) to classify Spotify songs into two genres: Underground Rap and Drum and Bass (dnb). I used a dataset from the Spotify API, containing 18 audio features of each song. The goal was to determine the best model for this classification task based on accuracy, sensitivity, and specificity.\n\n\nModel Comparison\n\nImplemented and tuned each model using grid search and cross-validation\nEvaluated model performance using accuracy, sensitivity, specificity, and F1 score\nVisualized model performance using confusion matrices and ROC-AUC curves\n\n\n\nResults\n\nRandom Forest model achieved the highest accuracy (92.1%) and F1 score (0.921)\nBagged Tree model showed the highest sensitivity (0.943)\nDecision Tree model had the highest specificity (0.933)\n\n\n#load required libaries\nlibrary(spotifyr) #API interaction\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(rsample)\nlibrary(recipes)\nlibrary(caret)\n\n\nData Preparation\n\n# read the online spotify data\ngenre &lt;- read_csv(\"genres_v2.csv\")\nplaylist &lt;- read_csv(\"playlists.csv\")\n\n\n#jon the data with id from genre and playlist\nspotify_data &lt;- genre %&gt;% \n                left_join(playlist, by =c(\"id\"= \"Playlist\"))\n\n# filter the data to include only the tracks of the two genres you'd like to use for the classification task\nspotify_data &lt;- spotify_data %&gt;% \n                filter(genre %in% c(\"Underground Rap\", \"dnb\")) %&gt;% \n                #rename underground rap to rap\n                mutate(genre = ifelse(genre == \"Underground Rap\", \"URap\", genre))\n\n\n\nData Exploration\n\n# remove columns that you won't be feeding model\nspotify &lt;- spotify_data  %&gt;%  select(song_name, genre, danceability: tempo) %&gt;% \n            #change genre to factor\n            mutate(genre = as.factor(genre))\n\n#find the top 10 most danaceable tracks in the dataset\ntop_20_songs &lt;- spotify %&gt;% \n     arrange(desc(danceability)) %&gt;% \n     head(20) %&gt;% \n     select(song_name, genre, danceability)\n\n# output table with interactivity features\nkableExtra:: kable(top_20_songs,\n                   caption = \"Top 20 most danceable tracks in the dataset\")\n\n\nTop 20 most danceable tracks in the dataset\n\n\nsong_name\ngenre\ndanceability\n\n\n\n\nPOP, LOCK & DROPDEAD\nURap\n0.985\n\n\nLoyaltyRunsDeepInDaLongRun\nURap\n0.985\n\n\nTwo Left Feet Flow\nURap\n0.984\n\n\nHate Your Guts\nURap\n0.983\n\n\nMugen Woe\nURap\n0.982\n\n\nThe 3\nURap\n0.980\n\n\nMavericks\nURap\n0.979\n\n\nTechnicolor\nURap\n0.977\n\n\nKillmonger\nURap\n0.977\n\n\nFunky Friday\nURap\n0.975\n\n\nNervous\nURap\n0.975\n\n\nGo to the Sto\nURap\n0.975\n\n\nBad Bad Bad (feat. Lil Baby)\nURap\n0.974\n\n\nAbraham Lincoln\nURap\n0.974\n\n\nPsycho Pass\nURap\n0.973\n\n\nWho the Fuck Is You\nURap\n0.972\n\n\nDottin Up\nURap\n0.971\n\n\nWorst Day of My Life\nURap\n0.971\n\n\nExcalibur\nURap\n0.970\n\n\nSexy\nURap\n0.970\n\n\n\n\n\n\n#difference between genres for some of the audio features\n#---drop song name, not required for models\nspotify &lt;- spotify %&gt;%  select(-song_name)\n\n#setup manual colors\ncolors &lt;- c(\"#7f58AF\", \"#64C5EB\", \"#E84D8A\", \"#FEB326\", \"lightblue\")\n\n#plot the data\nspotify %&gt;% \n  group_by(genre) %&gt;% \n  summarise(across(danceability:tempo, mean)) %&gt;% \n  pivot_longer(cols = danceability:tempo, names_to = \"audio_feature\", values_to = \"mean\") %&gt;% \n  ggplot(aes(x = genre, y = mean, fill = genre)) +\n  geom_col(position = \"dodge\") +\n  facet_wrap(~audio_feature, scales = \"free\") +\n  theme_classic() +\n  xlab(\"\") +\n  ylab(\"\") +\n  theme(legend.position = \"none\") +\n  scale_fill_manual(values = colors)+\n  #add caption\n  labs(caption = paste0(title = \"Source: Spotify API\")) + \n  #add title at the top center \n  ggtitle(\"Difference between genres for some of the audio features\")\n\n\n\n\n\n\n\n\n\n\n\n1.K-nearest neighbor\nThe k-nearest neighbors (KNN) method is a non-parametric classification algorithm used for recommendation systems. In the context of a Spotify dataset, the KNN algorithm can be employed to recommend songs or artists to a user based on the preferences of other users with similar listening histories. The algorithm calculates the distances between the target user and other users in the dataset based on their features, such as the artists or genres they have listened to. It then selects the k nearest neighbors, which are the users with the shortest distances to the target user, and recommends songs or artists that are popular among these neighbors.\n\n#load the libraries specific for this model\nlibrary(dplyr)    \nlibrary(ggplot2) #great plots\nlibrary(rsample)  #data splitting \nlibrary(recipes) #data preprocessing\nlibrary(skimr) #data exploration\nlibrary(tidymodels) #re-entering tidymodel mode\nlibrary(kknn) #knn modeling\nlibrary(caTools) #for splitting the data\n\n\n#split the data into training and testing set\nset.seed(123)\nsplit = sample.split(spotify$genre, SplitRatio = 0.7)\nspotify_split = initial_split(spotify)\ntrain = subset(spotify, split == TRUE)\ntest =  subset(spotify, split == FALSE)\n\n\n#specify the recipe\nknn_rec &lt;- recipe(genre ~., data = train) %&gt;%\n  step_dummy(all_nominal(), -all_outcomes(), one_hot = T) %&gt;%\n  step_normalize(all_numeric(), -all_outcomes())\n\n## knn spec\nknn_spec &lt;- nearest_neighbor(neighbors = 5) %&gt;%\n  set_engine(\"kknn\") %&gt;%\n  set_mode(\"classification\")\n\n#bake the data\n#baked_train &lt;- bake(knn_rec, train)\n\n\n#apply to testing set\n#baked_test &lt;- bake(knn_rec, test)\n\nTrain the model with 5 folds validation . The model is then tuned with grid search to find the best value of k. The model is then fit to the testing set and the performance is evaluated with confusion matrix, precision, recall and f1 score.\n\n# cross validation on the dataset\ncv_folds &lt;- vfold_cv(train, v = 5)\n\n# put all together in workflow\nknn_workflow &lt;- workflow() %&gt;%\n                add_recipe(knn_rec) %&gt;% \n                add_model(knn_spec)\n                \n\n#fit the resamples and carry out validation\nknn_res &lt;- knn_workflow %&gt;%\n           fit_resamples(resamples = cv_folds, \n           control = control_resamples(save_pred = TRUE))\n\n# find the best value of k\nknn_spec_tune &lt;- nearest_neighbor(neighbors = tune()) %&gt;%\n  set_mode(\"classification\") %&gt;%\n  set_engine(\"kknn\")\n\n# define a new workflow\nwf_knn_tune  &lt;- workflow() %&gt;%\n                add_model(knn_spec_tune) %&gt;% \n                add_recipe(knn_rec)\n  \n# tune the best model with grid search\nfit_knn_cv &lt;-  wf_knn_tune %&gt;%\n  tune_grid(cv_folds, grid = data.frame(neighbors = c(1, 5, seq(10, 100, 10))))\n\n#plot the fit knn cv with changing value of neighbours\nfit_knn_cv %&gt;% collect_metrics() %&gt;% \n  filter(.metric == \"roc_auc\") %&gt;% \n  ggplot(aes(x = neighbors, y = mean)) +\n  geom_point() +\n  geom_line() +\n  labs(title = \"Accuracy of KNN model with different values of k\",\n       x = \"Number of neighbors\",\n       y = \"Accuracy\") +\n  theme_minimal()\n\n\n\n\n\n\n\n# check the performance with collect_metrics\nfit_knn_cv %&gt;% collect_metrics()\n\n# A tibble: 36 √ó 7\n   neighbors .metric     .estimator   mean     n  std_err .config              \n       &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt; &lt;chr&gt;                \n 1         1 accuracy    binary     0.965      5 0.00170  Preprocessor1_Model01\n 2         1 brier_class binary     0.0351     5 0.00170  Preprocessor1_Model01\n 3         1 roc_auc     binary     0.965      5 0.00309  Preprocessor1_Model01\n 4         5 accuracy    binary     0.968      5 0.00176  Preprocessor1_Model02\n 5         5 brier_class binary     0.0258     5 0.00118  Preprocessor1_Model02\n 6         5 roc_auc     binary     0.989      5 0.00111  Preprocessor1_Model02\n 7        10 accuracy    binary     0.968      5 0.000903 Preprocessor1_Model03\n 8        10 brier_class binary     0.0247     5 0.000804 Preprocessor1_Model03\n 9        10 roc_auc     binary     0.993      5 0.000952 Preprocessor1_Model03\n10        20 accuracy    binary     0.967      5 0.00105  Preprocessor1_Model04\n# ‚Ñπ 26 more rows\n\n# create a final workflow\nfinal_wf &lt;- wf_knn_tune %&gt;%\n  finalize_workflow(select_best(fit_knn_cv, metric= \"accuracy\"))\n\n# fit the final model\nfinal_fit &lt;- final_wf %&gt;% fit(data = train)\n\n#predict to testing set\nspotify_pred &lt;- final_fit %&gt;% predict(new_data = test)\n\n# Write over 'final_fit' with this last_fit() approach\nfinal_fit &lt;- final_wf %&gt;% last_fit(spotify_split)\n\n# Collect metrics on the test data!\n\n\nfinal_fit %&gt;% collect_metrics()\n\n# A tibble: 3 √ó 4\n  .metric     .estimator .estimate .config             \n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy    binary        0.968  Preprocessor1_Model1\n2 roc_auc     binary        0.992  Preprocessor1_Model1\n3 brier_class binary        0.0237 Preprocessor1_Model1\n\n#bind genre from test data to the predicted data\nbind_test_pred &lt;- spotify_pred %&gt;% bind_cols(test)\n\n#plot the confusion matrix\nbind_test_pred %&gt;%\n  conf_mat(truth = genre, estimate = .pred_class) %&gt;% \n  autoplot(type = \"heatmap\") +\n  theme_minimal()+\n  #remove legend\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n#calculate precision, recall and f1 score\nknn_estimate &lt;- bind_test_pred %&gt;% \n                conf_mat(truth = genre, estimate = .pred_class) %&gt;% \n                summary() %&gt;% \n                head(4) %&gt;% \n                #rename .estimate to knn estimate\n                rename(\"knn estimate\" = .estimate)\n\n\n\n2.Decision tree\nDecision trees uses CART algorithm to split the data into two or more homogeneous sets. The algorithm uses the Gini index to create the splits. The model is then tuned with grid search to find the best hyperparameters. The model is then fit to the testing set and the performance is evaluated with confusion matrix, precision, recall and f1 score, as for knn model.\n\n# load the packages for decision trees\nlibrary(MASS)\nlibrary(doParallel)\nlibrary(vip)\n\n# methods using in class\ngenre_split &lt;-  initial_split(spotify)\ngenre_train &lt;-  training(genre_split)\ngenre_test  &lt;-  testing(genre_split)\n\n\n##Preprocess the data\ngenre_rec &lt;- recipe(genre ~., data = genre_train) %&gt;%\n  step_dummy(all_nominal(), -all_outcomes(), one_hot = TRUE) %&gt;%\n  step_normalize(all_numeric(), -all_outcomes())\n\n#new spec, tell the model that we are tuning hypermeter\ntree_spec_tune &lt;- decision_tree(\n  cost_complexity = tune(),\n  tree_depth = tune(),\n  min_n = tune()) %&gt;%\n  set_engine(\"rpart\") %&gt;%\n  set_mode(\"classification\")\n\nUse grid search to find the best hyperparameters and train on folded training set.\n\n# grid search the hyperparameters tuning \ntree_grid &lt;- grid_regular(cost_complexity(), tree_depth(), min_n(), levels = 5)\n\n#set up k-fold cv and use the model on this data\ngenre_cv = genre_train %&gt;% vfold_cv(v = 10)\ndoParallel::registerDoParallel() #build trees in parallel\n\n# setup the grid search\ntree_rs &lt;- tune_grid(tree_spec_tune, genre ~., \n                     resamples = genre_cv,\n                     grid = tree_grid,\n                     metrics = metric_set(accuracy))\n\nUse the workflow to finalize the model and also fit on the training set, and predicting on the test set.\n\n#finalize the model that is cross validate and hyperparameter is tuned\nfinal_tree &lt;- finalize_model(tree_spec_tune, select_best(tree_rs))\n\n#similar functions here.\nfinal_tree_fit &lt;- fit(final_tree, genre~., data = genre_train)\n\n#last_fit() fits on training data (like fit()), but then also evaluates on the test data.\nfinal_tree_result &lt;- last_fit(final_tree, genre~., genre_split)\nfinal_tree_result$.predictions\n\n[[1]]\n# A tibble: 2,211 √ó 6\n   .pred_dnb .pred_URap  .row .pred_class genre .config             \n       &lt;dbl&gt;      &lt;dbl&gt; &lt;int&gt; &lt;fct&gt;       &lt;fct&gt; &lt;chr&gt;               \n 1   0            1         2 URap        URap  Preprocessor1_Model1\n 2   0            1         3 URap        URap  Preprocessor1_Model1\n 3   0            1         4 URap        URap  Preprocessor1_Model1\n 4   0.00157      0.998     6 URap        URap  Preprocessor1_Model1\n 5   0            1         7 URap        URap  Preprocessor1_Model1\n 6   0.00157      0.998     8 URap        URap  Preprocessor1_Model1\n 7   0            1        14 URap        URap  Preprocessor1_Model1\n 8   0.00157      0.998    16 URap        URap  Preprocessor1_Model1\n 9   0.00157      0.998    17 URap        URap  Preprocessor1_Model1\n10   0.00157      0.998    20 URap        URap  Preprocessor1_Model1\n# ‚Ñπ 2,201 more rows\n\n\n\n#visualize the model\nfinal_tree_fit %&gt;%\n  vip(geom = \"col\", aesthetics = list(fill = \"midnightblue\", alpha = 0.8)) +\n  scale_y_continuous(expand = c(0,0))\n\n\n\n\n\n\n\n# how accurate is this model on the test data?\nfinal_tree_result %&gt;% collect_metrics()\n\n# A tibble: 3 √ó 4\n  .metric     .estimator .estimate .config             \n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy    binary        0.983  Preprocessor1_Model1\n2 roc_auc     binary        0.982  Preprocessor1_Model1\n3 brier_class binary        0.0166 Preprocessor1_Model1\n\n#plot the confusion matrix\nfinal_tree_result %&gt;% \n  collect_predictions() %&gt;% \n  conf_mat(truth = genre, estimate = .pred_class) %&gt;% \n  autoplot(type = \"heatmap\") +\n  theme_minimal()+\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n#plot precision, recall and f1 score\ndt_estimate &lt;- final_tree_result %&gt;% \n                collect_predictions() %&gt;% \n                conf_mat(truth = genre, estimate = .pred_class) %&gt;% \n                summary() %&gt;% \n                head(4) %&gt;% \n                #rename .estimate to deicsion tree estimate\n                rename(\"decision tree estimate\" = .estimate)\n\n\n\n3.Bagged tree\nBagging is a method of ensemble learning that combines the predictions of multiple models to improve the overall performance. It is different from two previous model in the sense that it is collection of ML models. It works by training multiple models on different subsets of the training data and then combining their predictions to make a final prediction. The bagged tree uses decision trees as the base model. The model is then tuned with grid search to find the best hyperparameters. The model is then fit to the testing set and the performance is evaluated with confusion matrix, precision, recall and f1 score, as for knn model and the decision tree model.\n\n# Helper packages\nlibrary(doParallel)  # for parallel backend to foreach\nlibrary(foreach)     # for parallel processing with for loops\nlibrary(caret)       # for general model fitting\nlibrary(rpart)       # for fitting decision trees\nlibrary(ipred)       # for bagging\nlibrary(baguette)    # for bagging\nlibrary(tidymodels)  # Assuming you have tidymodels installed\n\n# Methods using in class\ngenre_split &lt;- initial_split(spotify)\ngenre_train &lt;- training(genre_split)\ngenre_test  &lt;- testing(genre_split)\n\n## Preprocess the data\ngenre_rec &lt;- recipe(genre ~ ., data = genre_train) %&gt;%\n         step_dummy(all_nominal(), -all_outcomes(), one_hot = TRUE) %&gt;%\n         step_normalize(all_numeric(), -all_outcomes(),)\n\n# instatntiate bag model\nbag_model &lt;- bag_tree() %&gt;%\n     set_engine(\"rpart\", times = 100) %&gt;%\n     set_mode(\"classification\")\n\n# create a workflow\nbag_workflow &lt;- workflow() %&gt;%\n    add_model(bag_model) %&gt;%\n    add_recipe(genre_rec)\n\n\n##folds the data for validation set\ngenre_cv = genre_train %&gt;% vfold_cv(v = 10)\n\n# use tune grid to tune the model\nbag_tune &lt;- tune_grid(bag_workflow,\n                      resamples = genre_cv,\n                      grid = 10)\n\n# collect metrices\nbag_tune %&gt;% collect_metrics()\n\n# A tibble: 3 √ó 6\n  .metric     .estimator    mean     n  std_err .config             \n  &lt;chr&gt;       &lt;chr&gt;        &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy    binary     0.990      10 0.00156  Preprocessor1_Model1\n2 brier_class binary     0.00833    10 0.00112  Preprocessor1_Model1\n3 roc_auc     binary     0.998      10 0.000354 Preprocessor1_Model1\n\n\n\n#finalize the workflow\nbag_best = show_best(bag_tune, n = 1,  metric = \"roc_auc\")\n\n#fit the model\nfinalize_bag &lt;- finalize_workflow(bag_workflow, select_best(bag_tune, metric = \"roc_auc\" ))\n\n# fit the finalized model \nbag_fit &lt;- finalize_bag %&gt;% fit(genre_train)\n\n# predict the model on test\nbag_pred &lt;- bag_fit %&gt;% predict(genre_test) %&gt;% \n            bind_cols(genre_test)\n\n#predict the model with probaility values\nbag_pred_prob &lt;- bag_fit %&gt;% predict(genre_test, type = \"prob\") %&gt;% \n            bind_cols(genre_test)\n\n#model metrics and evaluation\naccuracy(bag_pred, truth = genre, estimate = .pred_class)\n\n# A tibble: 1 √ó 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.992\n\n#visualize the model\nbag_pred %&gt;%\n  conf_mat(truth = genre, estimate = .pred_class) %&gt;% \n  autoplot(type = \"heatmap\") +\n  theme_minimal()\n\n\n\n\n\n\n\n  #remove legend\n\n\ntheme(legend.position = \"none\")\n\nList of 1\n $ legend.position: chr \"none\"\n - attr(*, \"class\")= chr [1:2] \"theme\" \"gg\"\n - attr(*, \"complete\")= logi FALSE\n - attr(*, \"validate\")= logi TRUE\n\n#plot precision, recall and f1 score\nbag_estimate &lt;- bag_pred %&gt;% \n                conf_mat(truth = genre, estimate = .pred_class) %&gt;% \n                summary() %&gt;% \n                head(4) %&gt;% \n                #rename .estimate to bagged tree estimate\n                rename(\"bagged tree estimate\" = .estimate)\n\n\n\n4.Random Forest\nRandom forest is a popular ensemble learning method that combines the predictions of multiple decision trees to improve the overall performance. It is different from bagged tree in the sense that it uses a collection of decision trees as the base model, stochastically chosing number of columns too in training. That is not seen in general bagging model. The model is then tuned with grid search to find the best hyperparameters.\n\n# random forest with R\nlibrary(ranger) # for random forest\n\n# methods using in class\ngenre_split &lt;- initial_split(spotify)\ngenre_train &lt;- training(genre_split)\ngenre_test &lt;-  testing(genre_split)\n\n# fold the data for validation set\ncv_folds = vfold_cv(genre_train, v = 5)\n\n# create a previous recipe\ngenre_recipe &lt;- recipe(genre ~., data = genre_train) %&gt;%\n          step_dummy(all_nominal(), -all_outcomes(), one_hot = TRUE) %&gt;%\n          step_normalize(all_numeric(), -all_outcomes())\n\n# instantiate model\nrf_model &lt;- rand_forest(mtry = tune(), trees = tune()) %&gt;%\n         set_engine(\"ranger\") %&gt;%\n         set_mode(\"classification\")\n\n# create a workflow\nrf_workflow &lt;- workflow() %&gt;%\n         add_model(rf_model) %&gt;%\n         add_recipe(genre_recipe)\n\n# use tune grid to tune the model\nrf_tune &lt;- tune_grid(\n        rf_workflow,\n        resamples = cv_folds,\n        grid = 10)\n\n# collect metrices\nrf_tune %&gt;% collect_metrics()\n\n# A tibble: 30 √ó 8\n    mtry trees .metric     .estimator    mean     n   std_err .config           \n   &lt;int&gt; &lt;int&gt; &lt;chr&gt;       &lt;chr&gt;        &lt;dbl&gt; &lt;int&gt;     &lt;dbl&gt; &lt;chr&gt;             \n 1    10  1748 accuracy    binary     0.991       5 0.00108   Preprocessor1_Mod‚Ä¶\n 2    10  1748 brier_class binary     0.00757     5 0.000706  Preprocessor1_Mod‚Ä¶\n 3    10  1748 roc_auc     binary     0.999       5 0.000199  Preprocessor1_Mod‚Ä¶\n 4     7    69 accuracy    binary     0.992       5 0.000983  Preprocessor1_Mod‚Ä¶\n 5     7    69 brier_class binary     0.00661     5 0.000573  Preprocessor1_Mod‚Ä¶\n 6     7    69 roc_auc     binary     1.00        5 0.000158  Preprocessor1_Mod‚Ä¶\n 7     5  1242 accuracy    binary     0.993       5 0.000805  Preprocessor1_Mod‚Ä¶\n 8     5  1242 brier_class binary     0.00665     5 0.000585  Preprocessor1_Mod‚Ä¶\n 9     5  1242 roc_auc     binary     1.00        5 0.0000814 Preprocessor1_Mod‚Ä¶\n10     6   464 accuracy    binary     0.993       5 0.000873  Preprocessor1_Mod‚Ä¶\n# ‚Ñπ 20 more rows\n\n\n\n#finalize workflow\nrf_best = show_best(rf_tune, n = 1,  metric = \"roc_auc\")\n\n# finalize the model \nfinal_rand_forest &lt;- finalize_workflow(rf_workflow, select_best(rf_tune, metric = \"roc_auc\" ))\n\n#use this to predict the on test set\nfinal_rf_fit &lt;- final_rand_forest %&gt;% fit(genre_train)\n  \n# predict the model on test\nfinal_rf_pred &lt;- final_rf_fit %&gt;% predict(genre_test) %&gt;% \n            bind_cols(genre_test)\n\n#model metrics and evaluation\naccuracy(final_rf_pred, truth = genre, estimate = .pred_class)\n\n# A tibble: 1 √ó 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.993\n\n#visualize the model\nfinal_rf_pred %&gt;%\n  conf_mat(truth = genre, estimate = .pred_class) %&gt;% \n  autoplot(type = \"heatmap\") +\n  theme_minimal()+\n  #remove legend\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n#plot precision, recall and f1 score\nrf_estimate &lt;- final_rf_pred %&gt;% \n                conf_mat(truth = genre, estimate = .pred_class) %&gt;% \n                summary() %&gt;% \n                head(4) %&gt;% \n                #rename .estimate to random forest estimate\n                rename(\"random forest estimate\" = .estimate)\n\n\n\nModel Comparison\n\n#Compare the accuracy of all models and create a dataframe\n\n#bind all the estimates using left join\nestimate_table &lt;- knn_estimate %&gt;% \n                left_join(dt_estimate) %&gt;% \n                left_join(bag_estimate) %&gt;% \n                left_join(rf_estimate)\n\n\nestimate_table\n\n# A tibble: 4 √ó 6\n  .metric  .estimator `knn estimate` `decision tree estimate`\n  &lt;chr&gt;    &lt;chr&gt;               &lt;dbl&gt;                    &lt;dbl&gt;\n1 accuracy binary              0.970                    0.983\n2 kap      binary              0.933                    0.963\n3 sens     binary              0.970                    0.976\n4 spec     binary              0.970                    0.987\n# ‚Ñπ 2 more variables: `bagged tree estimate` &lt;dbl&gt;,\n#   `random forest estimate` &lt;dbl&gt;\n\n\n\n## plot the histogram of the accuracy of all models\nestimate_table %&gt;% \n  pivot_longer(cols = c(\"knn estimate\", \"decision tree estimate\", \"bagged tree estimate\", \"random forest estimate\"), names_to = \"model\", values_to = \"accuracy\") %&gt;% \n  ggplot(aes(x = model, y = accuracy, fill = model)) +\n  geom_col(position = \"dodge\") +\n  theme_minimal() +\n  xlab(\"\") +\n  ylab(\"accuracy\") +\n  theme(legend.position = \"none\") +\n  labs(caption = paste0(title = \"Source: Spotify API\")) + \n  ggtitle(\"Accuracy of all models\") + \n  #transform y axis to percentage multiplying by 100\n  scale_y_continuous(labels = scales::percent)+\n  #fill manual colors\n  scale_fill_manual(values = colors)\n\n\n\n\n\n\n\n\n\n\nConclusion\nThis project demonstrated the effectiveness of ensemble learning methods (Bagged Tree and Random Forest) in classifying Spotify songs into two genres. The results can be used to inform music recommendation systems or genre classification tasks. I showcased my skills in data preprocessing, model implementation, hyperparameter tuning, and performance evaluation."
  },
  {
    "objectID": "featured_projects/calcofi/index.html",
    "href": "featured_projects/calcofi/index.html",
    "title": "Using AI for Good: Helping non profits in predicting carbon calculation",
    "section": "",
    "text": "XYZ, name hidden for security purpose, was established in 1910 to investigate the ecological factors contributing to the collapse of the Pacific sardine population. Its extensive time series data provides invaluable insights into the long-term impacts of environmental changes on marine ecosystems and the communities reliant on them, not only within the California Current System but also extending to the North Pacific and beyond on an international scale.\nAs a part of this project, I am supporting the XYZ in predicting the ocean carbon values, which is a crucial part of the marine ecosystem. The dataset contains 12 columns, with 1 response variable and 11 predictors. The response variable is the dissolved inorganic carbon (DIC) concentration in the ocean, which is a key component of the marine carbon cycle. The predictors include various physical and chemical properties of the ocean, such as temperature, salinity, and oxygen concentration.\nThe goal of this project is to develop two machine learning models that can accurately predict the DIC concentration in the ocean based on the given predictors. This model will help better understand the factors influencing ocean carbon levels and make data-driven decisions to protect marine ecosystems.\n\nStepwise Flow of the Project\n\nLinear Regression Model\nFine Tuned XGBoost Model\n\n\n#hide all warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n#### Import all necessary libraries\n# import all required libraries\nimport numpy as np\nimport xgboost as xgb\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import cross_val_score, KFold\nfrom sklearn.linear_model import SGDRegressor\nfrom sklearn.linear_model import Ridge\nfrom sklearn.preprocessing import PolynomialFeatures\n\nRead the data and store ID:required during submission for validity check\n\n#-----read train and test set\ntrain_calcofi = pd.read_csv(\"data/train.csv\")\ntest_calcofi  = pd.read_csv(\"data/test.csv\")  \n\n#collect the test ids for testcalcofi, required for submission dataset\ntest_ids = test_calcofi.id\ntest_calcofi = test_calcofi.drop(columns=['id'])\n\n\nData cleaning\nA model is only as good as the data.This step ensures that columns names are standarized, and the columns with inappropriate values are removed. The data is also checked for missing values. Incase of missing values, they are not imputed, but dropped. The CALCOFI did not provide claer guidance on how value should be imputted. So the best decision is to drop the rows with missing values.\n\n#----inspect the head and take the insights of data\ntrain_calcofi.head()\n\n   id    Lat_Dec     Lon_Dec  ...  Salinity1  Temperature_degC      DIC\n0   1  34.385030 -120.665530  ...     34.198              7.82  2270.17\n1   2  31.418333 -121.998333  ...     34.074              7.15  2254.10\n2   3  34.385030 -120.665530  ...     33.537             11.68  2111.04\n3   4  33.482580 -122.533070  ...     34.048              8.36  2223.41\n4   5  31.414320 -121.997670  ...     34.117              7.57  2252.62\n\n[5 rows x 19 columns]\n\n#### Data cleaning and preprocessing\n#the column names are in snake case, change all to lowercase\ntrain_calcofi.columns = map(str.lower, train_calcofi.columns)\ntest_calcofi.columns = map(str.lower, test_calcofi.columns)\n\n\n#remove the unnamed:12 column\ntrain_calcofi = train_calcofi.drop(columns=['unnamed: 12'])\ntrain_calcofi.rename(columns={'ta1.x': 'ta1'}, inplace=True)\n\nThe data looks clean. Now, a relationships between columns must be established This helps in understanding the data, and also helps in feature selection. The next step below plots a correlation matrix. This will show correlated variables in the dataset.\nThe reason that the correlation matrix is plotted to see if linear regression can be useful. If the correlation matrix shows strong relationship between the response and predictors, then linear regression is a great algorithm. If not, then other models must be tested.\n\n#plot correlation matrix\ncorr_matrix = train_calcofi.corr()\n\n# Plot the heatmap\nplt.figure(figsize=(8, 6))\nsns.heatmap(corr_matrix, annot=False, cmap='coolwarm', fmt=\".1f\")\nplt.title('Correlation Heatmap')\nplt.show()\n\n\n\n\n\n\n\n\n\nLinear regression model\n\n# Select only the predictors columns, and change them to array\nX = train_calcofi.drop(columns=['dic', 'id'], axis=1)\n\n# Select only the response column and change it to array\ny = train_calcofi['dic']\n\n# Split the data into training, validation, and test sets\nX_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\nX_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n\n# Define the range of polynomial degrees to fit\ndegrees = range(1, 5)  # From 1 to 5\n\n# Initialize lists to store R^2 scores\ntrain_r2_scores = []\nval_r2_scores = []\n\n# Loop through each polynomial degree\nfor degree in degrees:\n    # Create a pipeline with PolynomialFeatures, StandardScaler, and LinearRegression\n    model_pipeline = make_pipeline(PolynomialFeatures(degree=degree), StandardScaler(), LinearRegression())\n\n    # Fit the model pipeline to the training data\n    model_pipeline.fit(X_train, y_train)\n\n    # Calculate R^2 on the training set\n    train_r2 = model_pipeline.score(X_train, y_train)\n    train_r2_scores.append(train_r2)\n\n    # Calculate R^2 on the validation set\n    val_r2 = model_pipeline.score(X_val, y_val)\n    val_r2_scores.append(val_r2)\n\n    # Print the results for each degree\n    print(f\"Degree: {degree}\")\n    print(f\"  R^2 on training set: {train_r2}\")\n    print(f\"  R^2 on validation set: {val_r2}\")\n    print(\"-\" * 40)\n\nPipeline(steps=[('polynomialfeatures', PolynomialFeatures(degree=4)),\n                ('standardscaler', StandardScaler()),\n                ('linearregression', LinearRegression())])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.¬†¬†Pipeline?Documentation for PipelineiFittedPipeline(steps=[('polynomialfeatures', PolynomialFeatures(degree=4)),\n                ('standardscaler', StandardScaler()),\n                ('linearregression', LinearRegression())]) ¬†PolynomialFeatures?Documentation for PolynomialFeaturesPolynomialFeatures(degree=4) ¬†StandardScaler?Documentation for StandardScalerStandardScaler() ¬†LinearRegression?Documentation for LinearRegressionLinearRegression() \n\n# Plotting the R^2 scores for training and validation sets\nplt.figure(figsize=(10, 6))\nplt.plot(degrees, train_r2_scores, label='Training R^2', marker='o')\nplt.plot(degrees, val_r2_scores, label='Validation R^2', marker='o')\nplt.xlabel('Polynomial Degree')\nplt.ylabel('R^2 Score')\nplt.title('Polynomial Degree vs. R^2 Score')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n#from the above sample, it is clear that either degree 1 or 2 is best\n# Define the polynomial degree\ndegree = 1\n\n# Define the range of regularization parameters (alpha values) to test\nalphas = np.logspace(-3, 3,  10)  # e.g., 10^-4 to 10^4\n\n# Initialize lists to store R^2 scores\ntrain_r2_scores = []\nval_r2_scores = []\n\n# Loop through each alpha value\nfor alpha in alphas:\n    # Create a pipeline with PolynomialFeatures, StandardScaler, and Ridge regression\n    model_pipeline = make_pipeline(PolynomialFeatures(degree=degree), StandardScaler(), Ridge(alpha=alpha))\n\n    # Fit the model pipeline to the training data\n    model_pipeline.fit(X_train, y_train)\n\n    # Calculate R^2 on the training set\n    train_r2 = model_pipeline.score(X_train, y_train)\n    train_r2_scores.append(train_r2)\n\n    # Calculate R^2 on the validation set\n    val_r2 = model_pipeline.score(X_val, y_val)\n    val_r2_scores.append(val_r2)\n\n    # Print the results for each alpha\n    print(f\"Alpha: {alpha}\")\n    print(f\"  R^2 on training set: {train_r2}\")\n    print(f\"  R^2 on validation set: {val_r2}\")\n    print(\"-\" * 40)\n\nPipeline(steps=[('polynomialfeatures', PolynomialFeatures(degree=1)),\n                ('standardscaler', StandardScaler()),\n                ('ridge', Ridge(alpha=np.float64(1000.0)))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.¬†¬†Pipeline?Documentation for PipelineiFittedPipeline(steps=[('polynomialfeatures', PolynomialFeatures(degree=1)),\n                ('standardscaler', StandardScaler()),\n                ('ridge', Ridge(alpha=np.float64(1000.0)))]) ¬†PolynomialFeatures?Documentation for PolynomialFeaturesPolynomialFeatures(degree=1) ¬†StandardScaler?Documentation for StandardScalerStandardScaler() ¬†Ridge?Documentation for RidgeRidge(alpha=np.float64(1000.0)) \n\n# Plotting the R^2 scores for training and validation sets\nplt.figure(figsize=(12, 6))\nplt.plot(alphas, train_r2_scores, label='Training R^2', marker='o', linestyle='-', color='b')\nplt.plot(alphas, val_r2_scores, label='Validation R^2', marker='o', linestyle='-', color='r')\nplt.xscale('log')  # Log scale for alpha values\nplt.xlabel('Regularization Parameter (Alpha)')\nplt.ylabel('R^2 Score')\nplt.title('Regularization Parameter (Alpha) vs. R^2 Score')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n#finalize the model\n# Split data into training and validation sets\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Define the polynomial degree and regularization parameter (lambda)\ndegree = 1\nalpha = 50  # Regularization parameter\n\n# Create and fit the model pipeline\nmodel_pipeline = make_pipeline(\n    PolynomialFeatures(degree=degree),\n    StandardScaler(),\n    Ridge(alpha=alpha)\n)\n\n# Fit the model to the training data\nmodel_pipeline.fit(X_train, y_train)\n\nPipeline(steps=[('polynomialfeatures', PolynomialFeatures(degree=1)),\n                ('standardscaler', StandardScaler()),\n                ('ridge', Ridge(alpha=50))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.¬†¬†Pipeline?Documentation for PipelineiFittedPipeline(steps=[('polynomialfeatures', PolynomialFeatures(degree=1)),\n                ('standardscaler', StandardScaler()),\n                ('ridge', Ridge(alpha=50))]) ¬†PolynomialFeatures?Documentation for PolynomialFeaturesPolynomialFeatures(degree=1) ¬†StandardScaler?Documentation for StandardScalerStandardScaler() ¬†Ridge?Documentation for RidgeRidge(alpha=50) \n\n# Evaluate the model\ntrain_r2 = model_pipeline.score(X_train, y_train)\nval_r2 = model_pipeline.score(X_val, y_val)\n\nprint(f\"Training R^2 score for Linear regression: {train_r2}\")\n\nTraining R^2 score for Linear regression: 0.9964417610035372\n\nprint(f\"Validation R^2 score for linear regression: {val_r2}\")\n\nValidation R^2 score for linear regression: 0.9964454374974873\n\n\nThe base linear regression model has worked well with 1 degree polynomial and low regularization parameters, with mean squared error of 36 on testing set, meaning ocean carbon values(DIC) was off by 36 point on average for the prediction test.\n\n\n\nCan XGBoost perform better ?\nThe next step involves using XGboost for making the prediction, Extreme Gradient Boosting, also called the queen of the ML models is one of the most robust models. Base XGBOOST model (no tuning: Out of Box model) Note:XGBoost works on its own object type, which is Dmatrix. So, datatype conversion is required.\n\n# Create regression matrices, this is requirement for xgboost model\ndtrain_reg = xgb.DMatrix(X_train, y_train, enable_categorical=True)\ndtest_reg =  xgb.DMatrix(X_test, y_test, enable_categorical=True)\n\n\n# use cross validation approach to catch the best boosting round\nn = 1000\n\nmodel_xgb = xgb.cv(\n   dtrain=dtrain_reg,\n   params = {},\n   num_boost_round= n,\n   nfold = 20, #number of folds for cross validation\n   verbose_eval=10, #record rmse every 10 interval\n   early_stopping_rounds = 5,\n   as_pandas = True#stop if there is no improvement in 5 consecutive rounds\n)\n\n[0] train-rmse:79.31111+0.22480 test-rmse:79.31059+4.50293\n[10]    train-rmse:4.32181+0.05969  test-rmse:6.83483+2.17431\n[20]    train-rmse:2.11548+0.06541  test-rmse:6.12915+2.26123\n[30]    train-rmse:1.64633+0.06258  test-rmse:6.04879+2.23034\n[40]    train-rmse:1.29777+0.05879  test-rmse:6.02162+2.23979\n[50]    train-rmse:1.02154+0.05521  test-rmse:5.99733+2.23518\n[53]    train-rmse:0.95655+0.05754  test-rmse:6.00205+2.23493\n\n\n# Extract the optimal number of boosting rounds\noptimal_boosting_rounds = model_xgb['test-rmse-mean'].idxmin()\n\n\n# #using validation sets during training\nevals = [(dtrain_reg, \"train\"), (dtest_reg, \"validation\")]\n\nmodel_xgb = xgb.train(\n   params={},\n   dtrain=dtrain_reg,\n   num_boost_round= optimal_boosting_rounds,\n   evals=evals,#print rmse for every iterations\n   verbose_eval=10, #record rmse every 10 interval\n   early_stopping_rounds = 5 #stop if there is no improvement in 5 consecutive rounds\n)\n\n[0] train-rmse:79.30237 validation-rmse:79.79093\n[10]    train-rmse:4.39108  validation-rmse:4.91840\n[20]    train-rmse:2.09378  validation-rmse:3.83489\n[30]    train-rmse:1.63902  validation-rmse:3.79141\n[40]    train-rmse:1.30465  validation-rmse:3.68415\n[48]    train-rmse:1.08249  validation-rmse:3.61313\n\n# #predict on the the test matrix\npreds = model_xgb.predict(dtest_reg)\n\n#check for rmse\nmse = mean_squared_error(y_test, preds, squared=False)\n\nprint(f\"MSE of the test model: {mse:.3f}\")\n\nMSE of the test model: 3.613\n\n\n**GRID TUNED XGBOOST MODEL\n\n# Define the parameter grid\ngbm_param_grid = {\n    'colsample_bytree': [0.5, 0.7, 0.9],\n    'n_estimators': [100, 200, 300, 1450],\n    'max_depth': [5, 7, 9],\n    'learning_rate': [0.001, 0.01]\n}\n\n#best hyperparameters based on running\ngbm_param_grid_set = {\n    'colsample_bytree': [0.5],\n    'n_estimators': [1450],\n    'max_depth': [5],\n    'learning_rate': [0.01]\n}\n\n# Instantiate the regressor\ngbm = xgb.XGBRegressor()\n\n# Instantiate GridSearchCV with seed\ngridsearch_mse = GridSearchCV(\n    param_grid=gbm_param_grid_set,\n    estimator=gbm,\n    scoring='neg_mean_squared_error',\n    cv=10,\n    verbose=1,\n)\n\n# Fit the gridmse\ngridsearch_mse.fit(X_train, y_train)\n\nGridSearchCV(cv=10,\n             estimator=XGBRegressor(base_score=None, booster=None,\n                                    callbacks=None, colsample_bylevel=None,\n                                    colsample_bynode=None,\n                                    colsample_bytree=None, device=None,\n                                    early_stopping_rounds=None,\n                                    enable_categorical=False, eval_metric=None,\n                                    feature_types=None, gamma=None,\n                                    grow_policy=None, importance_type=None,\n                                    interaction_constraints=None,\n                                    learning_rate=None,...\n                                    max_cat_to_onehot=None, max_delta_step=None,\n                                    max_depth=None, max_leaves=None,\n                                    min_child_weight=None, missing=nan,\n                                    monotone_constraints=None,\n                                    multi_strategy=None, n_estimators=None,\n                                    n_jobs=None, num_parallel_tree=None,\n                                    random_state=None, ...),\n             param_grid={'colsample_bytree': [0.5], 'learning_rate': [0.01],\n                         'max_depth': [5], 'n_estimators': [1450]},\n             scoring='neg_mean_squared_error', verbose=1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.¬†¬†GridSearchCV?Documentation for GridSearchCViFittedGridSearchCV(cv=10,\n             estimator=XGBRegressor(base_score=None, booster=None,\n                                    callbacks=None, colsample_bylevel=None,\n                                    colsample_bynode=None,\n                                    colsample_bytree=None, device=None,\n                                    early_stopping_rounds=None,\n                                    enable_categorical=False, eval_metric=None,\n                                    feature_types=None, gamma=None,\n                                    grow_policy=None, importance_type=None,\n                                    interaction_constraints=None,\n                                    learning_rate=None,...\n                                    max_cat_to_onehot=None, max_delta_step=None,\n                                    max_depth=None, max_leaves=None,\n                                    min_child_weight=None, missing=nan,\n                                    monotone_constraints=None,\n                                    multi_strategy=None, n_estimators=None,\n                                    n_jobs=None, num_parallel_tree=None,\n                                    random_state=None, ...),\n             param_grid={'colsample_bytree': [0.5], 'learning_rate': [0.01],\n                         'max_depth': [5], 'n_estimators': [1450]},\n             scoring='neg_mean_squared_error', verbose=1) best_estimator_: XGBRegressorXGBRegressor(base_score=None, booster=None, callbacks=None,\n             colsample_bylevel=None, colsample_bynode=None,\n             colsample_bytree=0.5, device=None, early_stopping_rounds=None,\n             enable_categorical=False, eval_metric=None, feature_types=None,\n             gamma=None, grow_policy=None, importance_type=None,\n             interaction_constraints=None, learning_rate=0.01, max_bin=None,\n             max_cat_threshold=None, max_cat_to_onehot=None,\n             max_delta_step=None, max_depth=5, max_leaves=None,\n             min_child_weight=None, missing=nan, monotone_constraints=None,\n             multi_strategy=None, n_estimators=1450, n_jobs=None,\n             num_parallel_tree=None, random_state=None, ...) XGBRegressorXGBRegressor(base_score=None, booster=None, callbacks=None,\n             colsample_bylevel=None, colsample_bynode=None,\n             colsample_bytree=0.5, device=None, early_stopping_rounds=None,\n             enable_categorical=False, eval_metric=None, feature_types=None,\n             gamma=None, grow_policy=None, importance_type=None,\n             interaction_constraints=None, learning_rate=0.01, max_bin=None,\n             max_cat_threshold=None, max_cat_to_onehot=None,\n             max_delta_step=None, max_depth=5, max_leaves=None,\n             min_child_weight=None, missing=nan, monotone_constraints=None,\n             multi_strategy=None, n_estimators=1450, n_jobs=None,\n             num_parallel_tree=None, random_state=None, ...) \n\n# Best estimator\nbest_estimator = gridsearch_mse.best_estimator_\n\n# Use the best estimator to make predictions on the test data\ny_pred = best_estimator.predict(X_test)\n\n# Calculate mean squared error\nmse_xgboost = mean_squared_error(y_test, y_pred)\nprint(\"Training Mean Squared Error:\", mse_xgboost)\n\nTraining Mean Squared Error: 8.730223465543306\n\n# Now, use the best estimator to make predictions on the validation data\ny_val_pred = best_estimator.predict(X_val)\n\n# Calculate mean squared error on the validation set\nmse_xgboost_val = mean_squared_error(y_val, y_val_pred)\nprint(\"Validation Mean Squared Error:\", mse_xgboost_val)\n\nValidation Mean Squared Error: 28.505288769482465\n\n# Get the model score on the validation set\nprint(f\"Model score on validation set: {best_estimator.score(X_val, y_val)}\")\n\nModel score on validation set: 0.9978675804165923\n\n\n\nprint(f\"Model score on validation setfor linear regression: {val_r2}\")\n\nModel score on validation setfor linear regression: 0.9964454374974873\n\nprint(f\"Model score on validation set for XGBoost: {best_estimator.score(X_val, y_val)}\")\n\nModel score on validation set for XGBoost: 0.9978675804165923\n\n\nThe XGBoost model has lower Bias, and high accuracy compared to the linear regression model. Thus, I suggest using the XGBOOST model for any new incoming data on ocean values."
  },
  {
    "objectID": "featured_projects/econometrics/index.html",
    "href": "featured_projects/econometrics/index.html",
    "title": "Did Smoking Mothers Have Lowered Weight Children?",
    "section": "",
    "text": "Summary and Conclusion\nThe analysis aimed to estimate the causal effect of maternal smoking during pregnancy on infant birth weight. Initial comparisons showed a significant difference in birth weights between infants of smoking and non-smoking mothers. However, confounding variables suggested that the groups were not comparable.\nPropensity score matching was applied to address these confounders. Post-matching tests indicated successful balancing of the treatment and control groups. The final analysis showed that maternal smoking causally reduces infant birth weight by an average of 13 grams, after accounting for confounding variables. This result highlights the importance of proper matching techniques in observational studies to draw valid causal inferences\n\n\nProject Begins:\nThe goal is to estimate the causal effect of maternal smoking during pregnancy on infant birth weight using the treatment ignorability assumptions. The data are taken from the National Natality Detail Files, and the extract ‚ÄúSMOKING_EDS241.csv‚Äù‚Äô is a random sample of all births in Pennsylvania during 1989-1991. Each observation is a mother-infant pair. The key variables are:\nThe outcome and treatment variables are:\nbirthwgt=birth weight of infant in grams\ntobacco=indicator for maternal smoking\nThe control variables are:\nmage (mother‚Äôs age), meduc (mother‚Äôs education), mblack (=1 if mother identifies as Black), alcohol (=1 if consumed alcohol during pregnancy), first (=1 if first child), diabete (=1 if mother diabetic), anemia (=1 if mother anemic)\n\nlibrary(here)\nlibrary(tidyverse)\nlibrary(knitr)\nlibrary(stargazer)\nlibrary(plm)\nlibrary(pglm)\nlibrary(dplyr)\nlibrary(MatchIt)\nlibrary(lmtest)\nlibrary(RItools)\nlibrary(sandwich)\nlibrary(estimatr)\nlibrary(Hmisc)\n\n\n# Load data\nsmoking_data &lt;- read_csv(here(\"featured_projects/econometrics/data/SMOKING_EDS241.csv\"))\n\n\nMean Differences, Assumptions, and Covariates\nFor conducting t-test, it is important to have population divided into treatment and control group. Since this experiment already has that information, I can segregate them into two tables and perform t-tests.\n\n## calculating difference using t.test when tobacco is 1 and 0\nsmoking_mothers = smoking_data %&gt;% filter(tobacco == 1)\nnon_smoking_mothers = smoking_data %&gt;% filter(tobacco == 0)\n\n#peform t-test to see if the difference is significant in other covariates\nt.test(smoking_mothers$birthwgt, non_smoking_mothers$birthwgt)\n\n\n    Welch Two Sample t-test\n\ndata:  smoking_mothers$birthwgt and non_smoking_mothers$birthwgt\nt = -58.932, df = 26945, p-value &lt; 2.2e-16\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -252.6727 -236.4060\nsample estimates:\nmean of x mean of y \n 3185.747  3430.286 \n\n\nThe mean difference is 244.539 grams between children from smoker and non-smoker mothers, which is significant at 5%. The assumptions for this mean difference to hold are ignorability (no other confounding variables influencing the outcome) and common support (there is sufficient overlap between the treatment and control group). If these assumptions are satisfied, I can infer the difference is actually due to smoking and not due to random chance.\nI can test those assumptions with t-tests for numeric covariates, and proportion test for categorical covariates The following code achives that:\n\n#set options to have maximum 5 decimal\noptions(digits=5)\n## For continuous variables I can use the t-test\n#t.test()\neducation &lt;- t.test(smoking_mothers$meduc, non_smoking_mothers$meduc)\nage &lt;- t.test(smoking_mothers$mage, non_smoking_mothers$mage)\nbirthwht &lt;- t.test(smoking_mothers$birthwgt, non_smoking_mothers$birthwgt)\n\n## For binary variables I should use the proportions test\n#prop.test()\nalcohol &lt;- prop.test(table(smoking_mothers$alcohol), table(non_smoking_mothers$alcohol))\nfirst_child &lt;-prop.test(table(smoking_mothers$first), table(non_smoking_mothers$first))\ndiabetes&lt;- prop.test(table(smoking_mothers$diabete), table(non_smoking_mothers$diabete))\nanaemia &lt;- prop.test(table(smoking_mothers$anemia),  table(non_smoking_mothers$anemia))\nblack   &lt;- prop.test(table(smoking_mothers$mblack),  table(non_smoking_mothers$mblack))\n\n#-------------- Covariate Calculations and Tables\n\n# create dataframe of coefficents from above results including\n# first column should be variable name, then mean of estimate for sample 1, then\n# mean of sample 2, then p values \ntable &lt;- data.frame(\n  variable = c(\"birthweight\", \"education\", \"age\", \"alcohol\", \"first_child\", \"diabetes\", \"anaemia\", \"black\"),\n  smoking_mothers = c(birthwht$estimate[1], education$estimate[1], age$estimate[1], \n                      sum(smoking_mothers$alcohol)/length(smoking_mothers$alcohol),  \n                      sum(smoking_mothers$first)/length(smoking_mothers$first), \n                      sum(smoking_mothers$diabete)/length(smoking_mothers$diabete), \n                      sum(smoking_mothers$anemia)/length(smoking_mothers$anemia), \n                      sum(smoking_mothers$mblack)/length(smoking_mothers$mblack)),\n  \n  non_smoking_mothers = c(birthwht$estimate[2], education$estimate[2], age$estimate[2], \n                          sum(non_smoking_mothers$alcohol)/length(non_smoking_mothers$alcohol),\n                          sum(non_smoking_mothers$first)/length(non_smoking_mothers$first),\n                          sum(non_smoking_mothers$diabete)/length(non_smoking_mothers$diabete), \n                          sum(non_smoking_mothers$anemia)/length(non_smoking_mothers$anemia), \n                          sum(non_smoking_mothers$mblack)/length(non_smoking_mothers$mblack)),\n  \n  p_value = round(c(birthwht$p.value, \n                    education$p.value, \n                    age$p.value, \n                    alcohol$p.value, \n                    first_child$p.value, \n                    diabetes$p.value, \n                    anaemia$p.value, \n                    black$p.value), 6))\n\nprint(table)\n\n     variable smoking_mothers non_smoking_mothers p_value\n1 birthweight      3.1857e+03          3.4303e+03       0\n2   education      1.1921e+01          1.3239e+01       0\n3         age      2.5539e+01          2.7453e+01       0\n4     alcohol      4.4182e-02          7.1033e-03       0\n5 first_child      3.6459e-01          4.3609e-01       0\n6    diabetes      1.7519e-02          1.7364e-02       0\n7     anaemia      1.4103e-02          7.8005e-03       0\n8       black      1.3541e-01          1.0863e-01       0\n\n\nInterpretation: The result shows that the population sample on treated and control group is dissimilar. The p values are less than .05, which means alternative hypothesis shoule be accepted. Alternative hypothesis for this test is: there is difference between treated and control group.\nSo far, I know there is already a difference in the two samples. But we can still quantify how much these covariates influence these biased samples. The following chunk will do that by calculating average treatment effects with a regression model for these biased groups.\n\n# ATE Regression univariate\ntobacco_univariate &lt;- lm(birthwgt ~ tobacco, data = smoking_data)\n\n# ATE with covariates\ntobacco_covariates &lt;- lm(birthwgt ~  tobacco + mage +  meduc +\n                           mblack + alcohol + first + diabete + anemia, \n                      data = smoking_data)\n\n## create combined table\nstargazer(tobacco_univariate, tobacco_covariates, type = \"text\", \n          out.header = TRUE, \n          title = \"Regression with and without controls\",\n          notes.label = \"significance level\")\n\n\nRegression with and without controls\n===========================================================================\n                                      Dependent variable:                  \n                    -------------------------------------------------------\n                                           birthwgt                        \n                                (1)                         (2)            \n---------------------------------------------------------------------------\ntobacco                     -244.540***                 -228.070***        \n                              (4.079)                     (4.177)          \n                                                                           \nmage                                                      -0.694*          \n                                                          (0.357)          \n                                                                           \nmeduc                                                    11.688***         \n                                                          (0.860)          \n                                                                           \nmblack                                                  -240.030***        \n                                                          (5.106)          \n                                                                           \nalcohol                                                  -77.350***        \n                                                          (13.465)         \n                                                                           \nfirst                                                    -96.944***        \n                                                          (3.447)          \n                                                                           \ndiabete                                                  73.228***         \n                                                          (12.104)         \n                                                                           \nanemia                                                     -4.796          \n                                                          (16.754)         \n                                                                           \nConstant                    3,430.300***                3,362.300***       \n                              (1.791)                     (11.927)         \n                                                                           \n---------------------------------------------------------------------------\nObservations                   94,173                      94,173          \nR2                             0.037                       0.072           \nAdjusted R2                    0.037                       0.072           \nResidual Std. Error     493.750 (df = 94171)        484.730 (df = 94164)   \nF Statistic         3,594.300*** (df = 1; 94171) 909.180*** (df = 8; 94164)\n===========================================================================\nsignificance level                              *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01\n\n\nIf I have to see the result above, seems like all covariates are responsible for change in birthweights in the smoking and non smoking mothers. But since the treated and control groups are dissimilar, it should not be inferred that these covariates are producing change.\nI can do one more test before I process the data to make them true. I can test if any of the covariates have a similar population between the treated and control groups already. I can use chi-squared tests among all these variables to see which one truly represents a similar population being compared.\n\n# perform balance test\nx &lt;- xBalance(tobacco ~ mage +  meduc + mblack + alcohol + first + diabete + anemia + birthwgt,  data = smoking_data,\n         report=c(\"std.diffs\",\"chisquare.test\", \"p.values\"))\n\n#use staragazer to present the results\nas.data.frame(x[1]) %&gt;% \n   #round last column to 5 digit\n  mutate_if(is.numeric, round, 5) %&gt;% \n  #rename columns based on number index\n  setNames(c( \"chi-Square/standard difference test\", \"p-value\"))\n\n         chi-Square/standard difference test p-value\nmage                                -0.36194  0.0000\nmeduc                               -0.64374  0.0000\nmblack                               0.08439  0.0000\nalcohol                              0.31525  0.0000\nfirst                               -0.14500  0.0000\ndiabete                              0.00119  0.8858\nanemia                               0.06670  0.0000\nbirthwgt                            -0.49527  0.0000\n\n\nOnly the diabetic covariate is similar between sample group at this point. What this means is that the population samples between treatment and control group are similar only in regards to diabetes.\nLets also calculate propensity estimation for this biased sample:\n\n\nPropensity Score Estimation for the biased Treated and control Groups\n\n## Propensity Scores estimation with logistic regression\nmother_prospensityscore &lt;- glm(tobacco ~  mage +  meduc +\n                           mblack + alcohol + first + diabete + anemia + birthwgt, data = smoking_data,\n                              family = binomial())\n\n## create a table of coefficients\nstargazer:: stargazer(mother_prospensityscore, type = \"text\")\n\n\n=============================================\n                      Dependent variable:    \n                  ---------------------------\n                            tobacco          \n---------------------------------------------\nmage                       -0.040***         \n                            (0.002)          \n                                             \nmeduc                      -0.288***         \n                            (0.005)          \n                                             \nmblack                     -0.361***         \n                            (0.028)          \n                                             \nalcohol                    1.962***          \n                            (0.062)          \n                                             \nfirst                      -0.460***         \n                            (0.020)          \n                                             \ndiabete                    0.229***          \n                            (0.067)          \n                                             \nanemia                     0.333***          \n                            (0.081)          \n                                             \nbirthwgt                   -0.001***         \n                           (0.00002)         \n                                             \nConstant                   6.456***          \n                            (0.090)          \n                                             \n---------------------------------------------\nObservations                94,173           \nLog Likelihood            -40,841.000        \nAkaike Inf. Crit.         81,699.000         \n=============================================\nNote:             *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01\n\n#create regression table dataframe based on mother_prospensityscore\n# Assuming you have a regression model object named 'model'\n# You would need to replace 'model' with the actual name of your model object\n\n\nmodel = mother_prospensityscore\n# Extract coefficients, standard errors, and p-values from the model\ncoefficients &lt;- coef(model)\nstandard_errors &lt;- sqrt(diag(vcov(model)))\np_values &lt;- summary(model)$coefficients[, 4]  # Extracting p-values\n\n# Define function to determine significance level\nget_significance_level &lt;- function(p_value) {\n  if (p_value &lt; 0.01) {\n    return('***')\n  } else if (p_value &lt; 0.05) {\n    return('**')\n  } else if (p_value &lt; 0.1) {\n    return('*')\n  } else {\n    return('')\n  }\n}\n\n# Get significance levels\nsignificance_levels &lt;- sapply(p_values, get_significance_level)\n\n# Create dataframe\ndf &lt;- data.frame(\n  Coefficient = coefficients,\n  Standard_error = round(standard_errors, 3),\n  Significance_level = significance_levels\n)\n\n# Print the dataframe\nprint(df)\n\n            Coefficient Standard_error Significance_level\n(Intercept)  6.45614093          0.090                ***\nmage        -0.04012754          0.002                ***\nmeduc       -0.28772570          0.005                ***\nmblack      -0.36100043          0.028                ***\nalcohol      1.96216177          0.062                ***\nfirst       -0.45972476          0.020                ***\ndiabete      0.22927944          0.067                ***\nanemia       0.33284394          0.081                ***\nbirthwgt    -0.00091614          0.000                ***\n\n\nThe covariates coefficients are strengths/weights of each variables that determines whether a unit will recieve the treatment. For example, the coefficient of mother‚Äôs age is 0.04, which means that for every unit increase in mother‚Äôs age, keeping all other covariates constant, the log odds of being in treatment group decreases by 0.02. This is statistically significant as evident by p value, which is much less than 0.05. All covariates in the model are significant which means they are all important in determining the treatment status for a unit X.\nAmong all covariates, alcohol has the greatest influence on whether a unit will recieve treatment, followed by first born child, which negative influence in being treatment group. These coefficients are all significant at 5% significance level.\nIn summary: what the above table is showing is that: all these covariates are responsible whether a unit falls into a treatment or control group. This is bad because an experiment should be completely random and should not influenced by anything. that means significance level for all those should have been more than 0.05.\nLook at the side by side histogram below. That is asymmetric. It should be made symmetric. and then I can run all analysis again .\n\n## use this logistic model to create a new column of prospensity scores for each observation\nsmoking_data$prospensity_scores &lt;- predict(mother_prospensityscore, type = \"response\")\n\n#round the scores to 2 decimal places\nsmoking_data$prospensity_scores &lt;- round(smoking_data$prospensity_scores, 2)\n\n## Histogram of PS before matching\nhistbackback(split(smoking_data$prospensity_scores, smoking_data$tobacco),\n             main= \"Propensity score before matching\",  \n             xlab=c(\"control\",  \"treatment\"))\n\n\n\n\n\n\n\n\nOverlap and its meaning Overlap refers to the degree of similarity or commonality between the treatment group (those who received the treatment being studied) and the control group (those who did not receive the treatment). The overlap is important because it is a necessary condition for the ignorability assumption to hold. If there is no overlap, then the treatment and control groups are so different that it is impossible to make causal inferences. The histogram above shows that there is a only some overlap between the treatment and control group, more individuals in the control group with lower propensity scores than in the treatment group. This is a violation of the Common Support assumption.\nBased on the analysis done so far. So, to resolve that I can select only those subjects that can create similarity in the samples.\nThat will involve following steps:\n-Calculate propensity score and match based on propensity values, then rerun all analysis as before. Propensity score calculates the probability that a unit will fall into treatment group.\n**Okay: lets begin the real analysis on unbiased data: The steps are:\n\nmatch the population and rerun ATE\nrerun average treatment effect on treated only\nperform more analysis matching samples by neighbour method, inverse weighted method\n\n\n\nSTEP 1. Matching the population with propensity score\nMatch treated/control mothers using your estimated propensity scores and nearest neighbor matching. Compare the balancing of pretreatment characteristics (covariates) between treated and non-treated units in the original dataset.\n\n## Nearest-neighbor Matching\nprospensity_score_matched &lt;- MatchIt::matchit(tobacco ~  mage +  meduc +\n                             mblack + alcohol + first + diabete + anemia + birthwgt, data = smoking_data, method = \"nearest\")\n\n## Covariate Imbalance post matching\nmatched_prospensity_dataset &lt;- match.data(prospensity_score_matched)\n\n# Drawing back to back histograms for propensity scores for treated and \n# non-treated after matching\nhistbackback(split(matched_prospensity_dataset$prospensity_scores,  matched_prospensity_dataset$tobacco),   main= \"Propensity\n        score   after   matching\",  xlab=c(\"control\",   \"treatment\"))\n\n\n\n\n\n\n\n\nPost matching, there is a better overlap between the treatment and control group. Units with high propensity score is matched with its counterparts and vice versa. This means the units we are comparing between the treatment and control group are similar. This helps us define counterfactuals of what would have happened to the treatment group if they were not treated.\n\n# the covariates between treated and non-treated that were used in the\n# estimation of the propensity scores\nxBalance(tobacco ~ mage +  meduc + mblack + alcohol + first + diabete + anemia + birthwgt, data = matched_prospensity_dataset,\n         report=c(\"std.diffs\",\"chisquare.test\", \"p.values\"))\n\n         strata():  unstrat    \n         stat      std.diff    \nvars                           \nmage                    0.0 ***\nmeduc                   0.0 ***\nmblack                  0.0 ** \nalcohol                 0.1 ***\nfirst                   0.0 ***\ndiabete                 0.0 .  \nanemia                  0.0    \nbirthwgt                0.0 *  \n---Overall Test---\n        chisquare df p.value\nunstrat       175  8   1e-33\n---\nSignif. codes:  0 '***' 0.001 '** ' 0.01 '*  ' 0.05 '.  ' 0.1 '   ' 1 \n\nxBalance(tobacco ~ mage +  meduc + mblack + alcohol + first + diabete + anemia + birthwgt, data = smoking_data,\n         report=c(\"std.diffs\",\"chisquare.test\", \"p.values\"))\n\n         strata():  unstrat     \n         stat      std.diff     \nvars                            \nmage                   -0.4 *** \nmeduc                  -0.6 *** \nmblack                  0.1 *** \nalcohol                 0.3 *** \nfirst                  -0.1 *** \ndiabete                 0.0     \nanemia                  0.1 *** \nbirthwgt               -0.5 *** \n---Overall Test---\n        chisquare df p.value\nunstrat     10298  8       0\n---\nSignif. codes:  0 '***' 0.001 '** ' 0.01 '*  ' 0.05 '.  ' 0.1 '   ' 1 \n\n\nAfter the matching, the nature and weight of the regression coefficients changed. Previously in unmatched data, I discussed that with increase age of mother, propensity score decreases. But post matching, the coefficients is showing that propensity score actually increases with increasing age of the mother. This is a sign that matching have accounted fixed effects in the observational dataset. Moreover, some covariates which were significant in determining the treatement status in the pre matching, are not significant anymore post matching. Like diabete, anemia, birthwgt, and mblack. Thus mother being black, having diabetes, and having anaemia, does not determine if she will receive the treatment or not.\nBut, this conclusion is not yet sufficient. What if I see this change only on treated population and not in control group ? The follwing step involves that:\n\n\nSTEP 2: Average treatment effect on treated group\nThis step is necessary because it can be more robust estimate. For example: this will compare the change before treatment and after treatment in the same units.\n\n## calculate ATE based on nearest neighbor matching\nsumdiff_data &lt;- matched_prospensity_dataset%&gt;%\n  group_by(subclass)%&gt;%\n  mutate(diff = birthwgt[tobacco==1]- birthwgt[tobacco==0])\n\ndif_in_treated &lt;- sum(sumdiff_data$diff)/2\n\nATT_weighted_count = 1/sum(matched_prospensity_dataset$tobacco) * dif_in_treated\nATT_weighted_count\n\n[1] -13.361\n\n\nFor the treated smoker mothers, the tobacco effects on their child birth weight is lower by 13 grams on average than for its counterfactual non smoking mothers. This means that the treatment has caused lower birth weight in the treated group of mothers. For any other units of population who shares similar covariates as treated group, the birth weight of their child will be lower by 13 grams on average if they were to start smoking tobacco.\nIs the conclusion final ? not yet. What if the propensity matching has drawbacks, which it does already. Propensity score matching is based on the fact that all covariates have similar influence in treatment. But, this does not always hold true. WLS matching fixes that.\n\n\nATE with WLS Matching\nThis step is necessary becuase the\n\n## Weighted least Squares (WLS) estimator Preparation\nsmoking_data &lt;- smoking_data %&gt;% \n  mutate(weights = tobacco / prospensity_scores + (1 - tobacco) / (1 - prospensity_scores))\n\n## Weighted least Squares (WLS) Estimates\nwls &lt;- lm(birthwgt ~ tobacco + mage +  meduc +\n                           mblack + alcohol + first + diabete + anemia, \n          data = smoking_data, weights = weights)\n\nsummary(wls)\n\n\nCall:\nlm(formula = birthwgt ~ tobacco + mage + meduc + mblack + alcohol + \n    first + diabete + anemia, data = smoking_data, weights = weights)\n\nWeighted Residuals:\n   Min     1Q Median     3Q    Max \n -7283   -374     32    396  12243 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 3122.734     11.584  269.58  &lt; 2e-16 ***\ntobacco        1.426      3.241    0.44     0.66    \nmage           0.219      0.358    0.61     0.54    \nmeduc         23.497      0.881   26.69  &lt; 2e-16 ***\nmblack      -215.695      4.978  -43.33  &lt; 2e-16 ***\nalcohol     -174.499     13.803  -12.64  &lt; 2e-16 ***\nfirst        -65.326      3.509  -18.61  &lt; 2e-16 ***\ndiabete       72.223     12.213    5.91  3.4e-09 ***\nanemia       -23.746     16.766   -1.42     0.16    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 698 on 94163 degrees of freedom\n  (1 observation deleted due to missingness)\nMultiple R-squared:  0.0369,    Adjusted R-squared:  0.0368 \nF-statistic:  451 on 8 and 94163 DF,  p-value: &lt;2e-16\n\n## Present results\n\nThe WLS matching is weighted based on propensity scores, meaning unit with higher similarity in covariates gets more weights. Based on this matching, the average birth weight of children for non smoker mother or control group is 3122.7338, keeping all other variables unchanged. This is statistically significant at 5% signficance. Of all the covariates, mblack covariates has greatest influence in outcome. i.e If the women identifies as black, the birth weight of the child is lower by 241 grams, which is significant. Besides, other covariates like drinking alcohol, first born child, and diabetes in covariates also significantly influence birthweight. However, tobacco appears insignificant at 5 percent signficance level. The model only explains 3 percent of variation given by R squared in birth weight of children. This means that the model is not a good fit for the data.\nThere are certain factors that influences the output more than other covariates. This was also shown by the coefficients in Balance estimates in previous step. When using ATT with propensity score matching, the model assumed that all covariates has equal influence in determining the treatment status. But in reality, this is not always true. The WLS matching takes weights of each covariates into account, thus providing different output than ATT. if all the covariates had same weights, we would have got same estimate using both ATT and WLS matching.\nIs the conclusion final ?\n-Statistically YES !"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "",
    "section": "",
    "text": "Areas of Proficiency\n  \n\n\n\n  \n     SUJAN BHATTARAI\n     Welcome to The Dataverse\n  \n  \n    \n      Areas of Proficiency\n    \n    \n      \n        Data Science\n        Enjoys analytical thinking, data-driven decisions, and content creation about Data Science\n      \n      \n        Machine Learning\n        Strong understanding of maths in ML algorithms: neural networks, tree based models, regressions\n      \n      \n        Causal Inference\n        Experience with First order effects, DID estimate, panel data, inverse weighted regression\n      \n      \n        Databases\n        Proficient at RDBMS, experience with snowflake and big data, concepts on HDFS and Hadoop \n      \n      \n        Modelling\n         Ran modelling simulations, randomize experimental designs, measure parameters uncertainties\n      \n      \n        Data Visualization\n         Knowledge of data visualization concepts, expert at ggplot and ggplot-children packages"
  },
  {
    "objectID": "certifications.html",
    "href": "certifications.html",
    "title": "",
    "section": "",
    "text": "Certificate Page\n    \n\n\n    \n    \n\n    \n    \n        \n            Data Science with R\n            \n                Fundamentals of R programming\n                Data manipulation with dplyr and tidyr\n                Data visualization using ggplot2\n                Statistical analysis and hypothesis testing\n                Machine learning with R\n            \n        \n        \n            \n        \n    \n\n    \n        \n            SAS programming\n            \n                SAS function, tables, Schemas\n                Data cleaning and analysis with SAS\n                Statistical analysis with SAS procedures\n                Creation of reports and graphics\n            \n        \n        \n            \n        \n    \n\n    \n        \n            Concepts on Big data\n            \n                Conceptual understanding of Big Data\n                Big Data technologies and ecosystems\n                Data processing and storage techniques\n                Big Data analytics and applications\n            \n        \n        \n            \n        \n    \n\n    \n        \n            Big Data Modelling\n            \n                Understanding of Big Data modeling techniques\n                Application of predictive analytics in Big Data\n                Hands-on experience with Hadoop\n                Designing and implementing Big Data solutions\n            \n        \n        \n            \n        \n    \n\n    \n        \n            Data Analyst with SQL\n            \n                SQL queries and RDBMS foundation\n                Data manipulation with SQL\n                Joining and aggregating data\n                Writing subqueries and CTEs\n            \n        \n        \n            \n        \n    \n\n    \n    \n     \n        \n            Hands-On Essentials: Data Warehouse\n            \n                Create, edit, and load snoflake Tables\n                Create and Use Snowflake resources\n                Create and edit COPY statements\n                Transform and parse CSV and JSON data"
  }
]