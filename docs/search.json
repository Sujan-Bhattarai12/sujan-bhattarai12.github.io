[
  {
    "objectID": "featured_projects.html",
    "href": "featured_projects.html",
    "title": "",
    "section": "",
    "text": "Using AI for Good: Helping non profits in predicting carbon calculation\n\n\n\n\n\n\nMachine Learning\n\n\nPython\n\n\n\n\n\n\n\n\n\nSujan Bhattarai\n\n\n\n\n\n\n\n\n\n\n\n\nDid Smoking Mothers Have Lowered Weight Children?\n\n\n\n\n\n\nCausal Inference\n\n\nR\n\n\n\n\n\n\n\n\n\nSujan Bhattarai\n\n\n\n\n\n\n\n\n\n\n\n\nSoccer players attributes comparison: Conclusion from Data visualization\n\n\n\n\n\n\nData Visualization\n\n\nR\n\n\n\n\n\n\n\n\n\nSujan Bhattarai\n\n\n\n\n\n\n\n\n\n\n\n\nExploring the Impact of Marketing Strategiese: A Deep Dive into EDA and A/B Testing\n\n\n\n\n\n\nABTest\n\n\nPython\n\n\n\n\n\n\n\n\n\nSujan Bhattarai\n\n\n\n\n\n\n\n\n\n\n\n\nComparing competing Machine Learning models in classifying Spotify songs\n\n\n\n\n\n\nMachine Learning\n\n\nR\n\n\n\n\n\n\n\n\n\nJan 20, 2024\n\n\nSujan Bhattarai\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html#updates",
    "href": "about.html#updates",
    "title": "\n\n",
    "section": "Updates:",
    "text": "Updates:\n\n📊 Practising SQL from HackerRank and learning DSA in LeetCode 🧠 Mastering neural network architecture and learning PyTorch from Coursera and DataCamp\n\n\nRecently completed:\n\n🖥 Worked on 4TB data on NCAR supercomputer, for developing Machine Learning model and a web-interactive dashboard 🚀 Learnt Big Data System with Apache Hive & Impala 📚 Read Machine Learning with Scikit Learn book by O’Reilly 🌐 Initiated a new Python project for a new blog in ML"
  },
  {
    "objectID": "about.html#tech-skills",
    "href": "about.html#tech-skills",
    "title": "\n\n",
    "section": "Tech Skills",
    "text": "Tech Skills\n\nPython 🐍\nR 📊\nSQL 💾\nSAS 📈\nBig Data 🌐\nMachine Learning 🤖\nData Visualization 📊\nGeospatial Analysis 🌍\nData Mining ⛏️\nHigh performance computing 🚀\nBash Scripting 📜\nWeb Development 🌐"
  },
  {
    "objectID": "about.html#blogs",
    "href": "about.html#blogs",
    "title": "\n\n",
    "section": "Blogs:",
    "text": "Blogs:\n\n\nForecasting Air-Quality Emission 🌬️\n\n\nGeospatial Analysis 🗺️\n\n\nArtificial Intelligence and Nature Conservation 🌿"
  },
  {
    "objectID": "about.html#statistics",
    "href": "about.html#statistics",
    "title": "\n\n",
    "section": "Statistics:",
    "text": "Statistics:\n 📈\n\nThanks for Visiting\nIf you have come this far, revise Hadoop CLI(HDFS DFS) commands, extremely useful if you are using SQL for Big data analysis) Apache_CLI"
  },
  {
    "objectID": "featured_projects/marketing/analysis.html",
    "href": "featured_projects/marketing/analysis.html",
    "title": "Exploring the Impact of Marketing Strategies Through Data Science: A Deep Dive into EDA and A/B Testing",
    "section": "",
    "text": "In the dynamic world of marketing, understanding customer behavior and optimizing strategies is paramount for success. As data scientists, we play a crucial role in extracting actionable insights from vast datasets. In this blog post, we will explore how exploratory data analysis (EDA) and A/B testing can be employed to enhance marketing campaigns, using a real-world marketing dataset.\n\nUnderstanding the Dataset\nOur dataset, collected from a marketing campaign, comprises various attributes, including user demographics, marketing channels, and conversion outcomes. The initial steps in our analysis involve loading and preprocessing the data to ensure its integrity and suitability for analysis\n\n\nExploring the Impact of Marketing Strategies Through Data Science: A Deep Dive into EDA and A/B Testing\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom scipy import stats  # for AB testing\n\n# Load the dataset\nmarketing = pd.read_csv(\"data/marketing.csv\")\nprint(marketing.head(5))\n\n      user_id date_served marketing_channel          variant converted  \\\n0  a100000029      1/1/18         House Ads  personalization      True   \n1  a100000030      1/1/18         House Ads  personalization      True   \n2  a100000031      1/1/18         House Ads  personalization      True   \n3  a100000032      1/1/18         House Ads  personalization      True   \n4  a100000033      1/1/18         House Ads  personalization      True   \n\n  language_displayed language_preferred    age_group date_subscribed  \\\n0            English            English   0-18 years          1/1/18   \n1            English            English  19-24 years          1/1/18   \n2            English            English  24-30 years          1/1/18   \n3            English            English  30-36 years          1/1/18   \n4            English            English  36-45 years          1/1/18   \n\n  date_canceled subscribing_channel is_retained  \n0           NaN           House Ads        True  \n1           NaN           House Ads        True  \n2           NaN           House Ads        True  \n3           NaN           House Ads        True  \n4           NaN           House Ads        True  \n\n\nTo facilitate our analysis, we map categorical marketing channels to numeric values. This transformation enables more efficient computations and visualizations.\n\n# Mapping channels to numeric values\nchannel_dict = {\n    \"House Ads\": 1, \"Instagram\": 2,\n    \"Facebook\": 3, \"Email\": 4, \"Push\": 5\n}\n\n# Map the channel to channel code\nmarketing['channel_code'] = marketing['subscribing_channel'].map(channel_dict)\n\n\nPerforming Exploratory Data Analysis (EDA)\nEDA is a critical step in understanding our data. By calculating key marketing metrics such as total users, retention rates, and conversion rates, we can assess the effectiveness of different marketing channels.\nTo automate the EDA process, we create a function to calculate conversion rates based on user segments. This function allows us to quickly assess the performance of different marketing strategies.\n\ndef conversion_rate(dataframe, column_names):\n    #total number of converted users\n    column_conv = dataframe[dataframe['converted']== True].groupby(column_names)['user_id'].nunique()\n    #total number of users\n    column_total= dataframe.groupby(column_names)['user_id'].nunique()\n    #conversion rate\n    conversion_rate = column_conv/column_total\n    #fill missing values with zero\n    conversion_rate = conversion_rate.fillna(0)\n    return conversion_rate\n\nUsing this function, we can calculate conversion rates by age group and visualize the results to identify trends in user engagement.\n\n# calculate conversion rate by age group\nage_group_conv = conversion_rate(marketing, ['date_served', 'age_group'])\n\n#unstack and create a dataframe\nage_group_df = pd.DataFrame(age_group_conv.unstack(level = 1))\n\n#visualize conversion by age group\nage_group_df.plot()\n\n&lt;Axes: xlabel='date_served'&gt;\n\n\n\n\n\n\n\n\n\n\n# a function that should produce plot based on grouping based on one column\ndef plotting_conv(dataframe):\n    for column in dataframe:\n        # Plot column by dataframe's index\n        plt.plot(dataframe.index, dataframe[column])\n        plt.title('Daily ' + str(column) + ' Conversion Rate', size=16)\n        plt.xlabel('Date', size=14)\n        \n        # Set x-axis labels to vertical\n        plt.xticks(rotation=90)\n\n        # Show plot\n        plt.show()\n        plt.clf()\n\n\n#calculate conversion rate by dateserved and age group\nage_group_conv = conversion_rate(marketing, ['date_served', 'age_group'])\n\n#unstakc age_group_conv and craete a dataframe\nage_group_df = pd.DataFrame(age_group_conv.unstack(level=1))\n\n#plot the results\nplotting_conv(age_group_df)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n\nAnalyzing the Impact of the Day of the Week\nUnderstanding temporal patterns in marketing effectiveness can provide valuable insights. We examine whether the day of the week affects conversion rates, especially for House Ads, which often see varying performance based on timing.\n\n#calculate conversion rate by date_served and channel\ndaily_conv_channel = conversion_rate(marketing, ['date_served', 'marketing_channel'])\nprint(daily_conv_channel)\n\ndate_served  marketing_channel\n2018-01-01   Email                1.000000\n             Facebook             0.117647\n             House Ads            0.084656\n             Instagram            0.106667\n             Push                 0.083333\n                                    ...   \n2018-01-31   Email                1.000000\n             Facebook             0.078947\n             House Ads            0.038217\n             Instagram            0.041096\n             Push                 0.052632\nName: user_id, Length: 155, dtype: float64\n\n\n\n#add day of week column to marketing\nmarketing['date_served'] = pd.to_datetime(marketing['date_served'])\nmarketing['DoW_served'] = marketing['date_served'].dt.dayofweek\n\n#calculate conversion rate by day of week\nDoW_conversion = conversion_rate(marketing, ['DoW_served', 'marketing_channel'])\n\n#unstack channels\nDoW_df = pd.DataFrame(DoW_conversion.unstack(level = 1))\n\n#plot conversion rate by day of week\nDoW_df.plot()\nplt.title('Conversion rate by day of week')\nplt.ylim(0)\nplt.show()\n\n\n\n\n\n\n\n\n\n\nSegmenting Users for Deeper Insights\nTo better understand how language preferences impact conversion, we segment our data by the displayed language and examine conversion rates.\n\n#Isolate hte rows where marking channel is house ads\nhouse_ads = marketing[marketing['marketing_channel']== 'House Ads']\n\n#calculate the conversion by date served and language displayed\nconv_lang_channel = conversion_rate(house_ads, ['date_served', 'language_displayed'])\n\n#unstack conv_lang_channel\nconv_lang_df = pd.DataFrame(conv_lang_channel.unstack(level=1))\n\n#use plotting fucntions to display results\nplotting_conv(conv_lang_df)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\nWe can also assess whether users received messages in their preferred language, revealing further insights into potential barriers to conversion.\n\n# Add the new column for language correctness\nhouse_ads['is_correct_lang'] = np.where(house_ads['language_preferred'] == house_ads['language_displayed'], 'Yes', 'No')\n\n# Group by date served and language correctness\nlanguage_check = house_ads.groupby(['date_served', 'is_correct_lang'])['is_correct_lang'].count()\nlanguage_check_df = pd.DataFrame(language_check.unstack(level=1)).fillna(0)\n\n# Print results and calculate percentage of correct language\nlanguage_check_df['pct'] = language_check_df['Yes'] / language_check_df.sum(axis=1)\n\n# Plot results\nplt.plot(language_check_df.index.values, language_check_df['pct'])\nplt.title('Correct Language Display Rate Over Time')\nplt.xlabel('Date')\n# Set x-axis labels to vertical\nplt.xticks(rotation=90)\nplt.ylabel('Percentage')\nplt.show()\n\n/var/folders/43/hk5fpcsd4gl7mt5ggvb968g80000gn/T/ipykernel_57718/92776872.py:2: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  house_ads['is_correct_lang'] = np.where(house_ads['language_preferred'] == house_ads['language_displayed'], 'Yes', 'No')\n\n\n\n\n\n\n\n\n\nSetting up conversion indexes\n\n#calcualte pre_error conversion rate\nhouse_ads_bug = house_ads[house_ads['date_served'] &lt; '2018-01-11']\nlang_conv = conversion_rate(house_ads_bug, ['language_displayed'])\n\n#Index other language conversion rate against English\nspanish_index = lang_conv['Spanish']/lang_conv['English']\narabic_index = lang_conv['Arabic']/lang_conv['English']\ngerman_index = lang_conv['German']/lang_conv['English']\n\nprint(\"Spanish index:\", spanish_index)\nprint(\"Arabic index:\", arabic_index)\nprint(\"German index:\", german_index)\n\nSpanish index: 1.681924882629108\nArabic index: 5.045774647887324\nGerman index: 4.485133020344287\n\n\n\n#Group house_ads by date and language\nconverted = house_ads.groupby(['date_served', 'language_preferred']).agg({'user_id': 'nunique', 'converted': 'sum'})\n\n#unstacked convereted\nconverted_df = pd.DataFrame(converted.unstack(level = 1))\n\n\n# Create English conversion rate column for affected period\nconverted_df['english_conv_rate'] = converted_df.loc['2018-01-11':'2018-01-31'][('converted','English')]/converted_df.loc['2018-01-11':'2018-01-31'][('user_id','English')]\n\n# Create expected conversion rates for each language\nconverted_df['expected_spanish_rate'] = converted_df['english_conv_rate']*spanish_index\nconverted_df['expected_arabic_rate'] = converted_df['english_conv_rate']*arabic_index\nconverted_df['expected_german_rate'] = converted_df['english_conv_rate']*german_index\n\n# Multiply number of users by the expected conversion rate\nconverted_df['expected_spanish_conv'] = converted_df['expected_spanish_rate']/100*converted_df[('user_id','Spanish')]\nconverted_df['expected_arabic_conv'] = converted_df['expected_arabic_rate']/100*converted_df[('user_id','Arabic')]\nconverted_df['expected_german_conv'] = converted_df['expected_german_rate']/100*converted_df[('user_id','German')]\n\n\n# use .loc to slice only the relevant dates\nconverted = converted_df.loc['2018-01-11':'2018-01-31']\n\n#sum expect subscribers for each language\nexpected_subs = converted['expected_spanish_conv'].sum() + converted['expected_arabic_conv'].sum() + converted['expected_german_conv'].sum()\n\n#calculate how many subscribers we actually got\nactual_subs = converted[('converted', 'Spanish')].sum()  + converted[('converted', 'Arabic')].sum() + converted[('converted', 'German')].sum()\n\n\n#substract how many subscribers we got despite the bug\nlost_subs = expected_subs - actual_subs\nprint(lost_subs)\n\n-25.495425075261792\n\n\n\n\nA/B Testing: Evaluating Email Campaigns\nTo assess the effectiveness of our marketing strategies, we conduct an A/B test on our email campaigns. By segmenting our users into control and personalization groups, we can evaluate which approach yields better conversion rates.\n\n# Subset the DataFrame for email marketing\nemail = marketing[marketing['marketing_channel'] == 'Email']\n\n# Group the email DataFrame by variant\nalloc = email.groupby(['variant'])['user_id'].nunique()\n\n# Plot test allocation\nalloc.plot(kind='bar')\nplt.title('Email Personalization Test Allocation')\nplt.ylabel('# Participants')\nplt.show()\n\n# Group marketing by user_id and variant\nsubscribers = email.groupby(['user_id', 'variant'])['converted'].max()\nsubscribers_df = pd.DataFrame(subscribers.unstack(level=1))\n\n# Calculate conversion rates\ncontrol = subscribers_df['control'].dropna()\npersonalization = subscribers_df['personalization'].dropna()\n\nprint('Control conversion rate:', np.mean(control))\nprint('Personalization conversion rate:', np.mean(personalization))\n\n\n\n\n\n\n\n\nControl conversion rate: 0.2814814814814815\nPersonalization conversion rate: 0.3908450704225352\n\n\nTo evaluate the effectiveness of personalization, we define a function to calculate the lift in conversion rates between the two groups.\n\ndef lift(a,b):\n    # Calcuate the mean of a and b \n    a_mean = np.mean(a)\n    b_mean = np.mean(b)\n    \n    # Calculate the lift using a_mean and b_mean\n    lift = (b_mean-a_mean)/a_mean\n  \n    return str(round(lift*100, 2)) + '%'\n  \n# Print lift() with control and personalization as inputs\nprint(lift(control, personalization))\n\n38.85%\n\n\n\n\nSegmenting for Targeted Insights in A/B Testing\nFinally, we can perform segmented A/B testing based on various user demographics, such as language displayed and age group, allowing us to uncover nuanced insights into user behavior.\n\ndef ab_segmentation(segment):\n    for subsegment in np.unique(marketing[segment].values):\n        print(f'Segment: {subsegment}')\n        \n        email = marketing[(marketing['marketing_channel'] == 'Email') & (marketing[segment] == subsegment)]\n        email.loc[:, 'converted'] = pd.to_numeric(email['converted'], errors='coerce')\n        \n        subscribers = email.groupby(['user_id', 'variant'])['converted'].max()\n        subscribers = pd.DataFrame(subscribers.unstack(level=1))\n        \n        control = pd.to_numeric(subscribers['control'], errors='coerce').dropna()\n        personalization = pd.to_numeric(subscribers['personalization'], errors='coerce').dropna()\n\n        if len(control) &gt; 0 and len(personalization) &gt; 0:\n            print('Lift:', lift(control, personalization))\n            print('T-statistic:', stats.ttest_ind(control, personalization), '\\n\\n')\n        else:\n            print('Not enough data to perform t-test for', subsegment)\n\n# Segmenting based on language displayed and age group\nab_segmentation('language_displayed')\nab_segmentation('age_group')\n\nSegment: Arabic\nLift: 50.0%\nT-statistic: TtestResult(statistic=np.float64(-0.5773502691896255), pvalue=np.float64(0.5795840000000001), df=np.float64(8.0)) \n\n\nSegment: English\nLift: 39.0%\nT-statistic: TtestResult(statistic=np.float64(-2.2183598646203215), pvalue=np.float64(0.026991701290720503), df=np.float64(486.0)) \n\n\nSegment: German\nLift: -1.62%\nT-statistic: TtestResult(statistic=np.float64(0.19100834180787182), pvalue=np.float64(0.8494394170062678), df=np.float64(42.0)) \n\n\nSegment: Spanish\nLift: 166.67%\nT-statistic: TtestResult(statistic=np.float64(-2.3570226039551585), pvalue=np.float64(0.04015671811047753), df=np.float64(10.0)) \n\n\nSegment: 0-18 years\nLift: 121.4%\nT-statistic: TtestResult(statistic=np.float64(-2.966044912142212), pvalue=np.float64(0.003872449439129706), df=np.float64(89.0)) \n\n\nSegment: 19-24 years\nLift: 106.24%\nT-statistic: TtestResult(statistic=np.float64(-3.0317943847866697), pvalue=np.float64(0.0030623836114689195), df=np.float64(105.0)) \n\n\nSegment: 24-30 years\nLift: 161.19%\nT-statistic: TtestResult(statistic=np.float64(-3.861539544326876), pvalue=np.float64(0.00018743381094867337), df=np.float64(114.0)) \n\n\nSegment: 30-36 years\nLift: -100.0%\nT-statistic: TtestResult(statistic=np.float64(3.185906464414798), pvalue=np.float64(0.002323848743176535), df=np.float64(58.0)) \n\n\nSegment: 36-45 years\nLift: -85.23%\nT-statistic: TtestResult(statistic=np.float64(2.431790127931851), pvalue=np.float64(0.017975686009788244), df=np.float64(61.0)) \n\n\nSegment: 45-55 years\nLift: -72.22%\nT-statistic: TtestResult(statistic=np.float64(2.0654991273179326), pvalue=np.float64(0.04306233968820124), df=np.float64(62.0)) \n\n\nSegment: 55+ years\nLift: -100.0%\nT-statistic: TtestResult(statistic=np.float64(3.326565456420339), pvalue=np.float64(0.0016358623456360468), df=np.float64(51.0)) \n\n\n\n\n/opt/anaconda3/envs/data-science/lib/python3.12/site-packages/scipy/stats/_axis_nan_policy.py:573: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n  res = hypotest_fun_out(*samples, **kwds)\n\n\n\n\nConclusion\nThrough EDA and A/B testing, data scientists can derive meaningful insights from marketing datasets, informing strategic decisions and optimizing marketing campaigns. By leveraging data analysis, we empower organizations to better understand their audiences and enhance their marketing efforts. This comprehensive approach not only drives engagement but also boosts conversion rates, ultimately leading to a more effective marketing strategy.\nIn an era where data is king, the role of the data scientist is not just to analyze data but to transform insights into actionable strategies that propel businesses forward."
  },
  {
    "objectID": "featured_projects/data_viz/index.html",
    "href": "featured_projects/data_viz/index.html",
    "title": "Soccer players attributes comparison: Conclusion from Data visualization",
    "section": "",
    "text": "Introduction about the project\nThis project aims to create infographics comparing attributes of soccer players, sourced from a European soccer database available on Kaggle. The database contains data on over 10,000 players. Using R exclusively, the visualization focuses on attributes such as shooting, finishing, strength, age, etc. All tasks, from fetching the data to adding text and aligning elements, are performed using R packages. Special credit is given to the UFO alien template, accessible at UFO link\nImportant Note: The ggplot package version used in this project is 3.4.4, and ggtern version 3.0.0 is utilized. It’s crucial to consider that modern versions of ggplot might conflict when used in combination with other packages employed here.\n\n\nData Wrangling\nThe datasets available on Kaggle are stored in SQLite and require joins to build a complete dataset. However, for this project, data from a single table is sufficient. Nevertheless, for future use, datasets will be combined if necessary.\n\n#read the csv file\nplayer_attr &lt;- read_csv(here(\"featured_projects/data_viz/player_attr.csv\"))\n\nThe dataset includes a column named “overall ratings,” representing the FIFA average rating from 2008 to 2016. This column will be utilized to categorize players into Advanced, Intermediate, and Novice. It’s important to note that the term “Novice” is used for classification purposes and does not undermine any players; it’s simply a classification based on personal preference.\n\n#---classify the players based on their overall rating\nplayer_attr &lt;- player_attr %&gt;% \n  mutate(player_class = ifelse(overall_rating &gt;= 85, \"Advanced\", \n                                 ifelse(overall_rating &gt;= 70 & overall_rating &lt; 85, \"Intermediate\", \"Novice\")))\n\nThe project will compare the following attributes of players. The hypothesis is whether there exists a difference in attributes between the world’s renowned top performers and players who do not enjoy the same level of popularity.\n\n# filter required columns from the dataset\nplayer_data &lt;- player_attr %&gt;% \n        select(player_name, player_class, crossing, agility, \n               dribbling, finishing, aggression, balance, strength, stamina)\n\nFrom the database containing over 10,000 players, only a subset is required. A random sample of 1000 players will be drawn from the intermediate and novice categories, while all players from the advanced category will be included, given that there are already fewer than 1000 advanced category players.\n\n#---filter only good players from the player_data datase\ngood_players &lt;- player_data %&gt;% filter(player_class == \"Advanced\")\n\n#---filter average and bad players from the player_data dataset\nset.seed(123)\naverage_bad_players &lt;- player_data %&gt;% filter(player_class != \"Advanced\") %&gt;% \n  #sample only 1000 from average and bad players\n  group_by(player_class) %&gt;% \n  #randomly select 1000 players from each class and always include the player Van dijk in the sample\n  sample_n(1000)\n\n#good defender van dijk\nvan_dijk &lt;- player_data %&gt;% filter(player_name == \"Virgil van Dijk\")\n\n#---combine the good_players and average_bad_players datasets\nclean_player_data &lt;- bind_rows(good_players, average_bad_players, van_dijk)\n\n\n\nPlot 1: Radar plot for common attributes comparison\n\n#summarize the data and plot the summary output with radar chart\nradar_data &lt;- clean_player_data %&gt;% \n  group_by(player_class) %&gt;% \n  select(-player_name,  player_class) %&gt;%\n  summarise_all(mean, na.rm = TRUE) %&gt;% \n  arrange(ifelse(player_class == \"Advanced\", 1, ifelse(player_class == \"Intermediate\", 2, 3))) %&gt;% \n  #arrange so that corssing, finishing, driblling and agility are away from each other\n  rename(\"finish\"= \"finishing\") %&gt;% \n  select(player_class, crossing, aggression, agility, balance, dribbling, stamina, finish, strength)\n\n#create custom color paletteA\ncolors &lt;- c(\"#7f58AF\", \"#64C5EB\", \"#E84D8A\", \"#FEB326\", \"lightblue\")\n\nradar_plot &lt;- ggradar::ggradar(radar_data,\n        grid.min = 0,\n        grid.max =  100,\n        grid.mid = 50,\n        axis.label.size = 22,\n        label.centre.y = FALSE,\n        group.line.width = 1,\n        gridline.mid.colour = \"#27593d\",\n        group.point.size  = 2,\n        grid.label.size = 20,\n        group.colours = c(\"#7f58AF\", \"#64C5EB\", \"#E84D8A\"),\n        background.circle.colour = \"white\",\n        legend.title  = \"Players proficiency\",\n        legend.text.size = 24,\n        font.radar = \"serif\") +\n  \n  #make the background theme white\n  theme_void() +\n  #remove y axis label\n  theme(axis.text.y = element_blank())+\n  #remove x axis label\n  theme(axis.text.x = element_blank())+\n  #remove all grid lines \n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())+\n  #remove legend\n  theme(legend.position = \"none\")\n\n#save this as png\nggsave(plot = radar_plot, filename = \"image/radar_plot.png\", height = 6, width = 6, scale = 1.1, dpi= 300)\n\n#Include the radar plot in the rmd\n#knitr::include_graphics(\"image/radar_plot.png\")\nknitr::include_graphics(\"static_image/radar.png\")\n\n\n\n\n\n\n\n\n\n\nPlot 2: Stat-halfeye plot for age distribution\n\nlibrary(ggdist)\n##eye plot\n#plot lefeye plot using stat_halfeye\n\ndata &lt;-player_attr %&gt;% \n  mutate(birthday = as.Date(birthday)) %&gt;% \n  mutate(birthday = as.numeric(format(birthday, \"%Y\"))) %&gt;%  #calculate the age\n  mutate(age = 2016 - birthday)\n\n#plot the half eye\neye_plot &lt;- ggplot(data, aes(x = age, y = player_class,  fill = player_class))+\n  stat_halfeye(alpha = 0.7, color = \"black\") +\n  theme(panel.background = element_blank(),\n               panel.grid = element_blank(),\n               legend.position = \"none\",\n               plot.background = element_blank(),\n               panel.border = element_blank(),\n               axis.title.x = element_blank(),\n               axis.line.y = element_line())+\n  theme(legend.position = \"none\",\n        axis.text = element_text(size = 60),\n        axis.title = element_text(size = 90),\n        text = element_text(family = \"serif\"),\n        panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank()) +\n  scale_fill_manual(values = colors) +\n  theme(axis.text.x = element_blank()) +\n  coord_flip() +\n  xlab(\"\") +\n  ylab(\"\")+\n  theme(panel.background = element_blank())\n  \n#save as ggplot\nggplot2::ggsave(plot = eye_plot, filename = \"image/eye_plot.png\", height = 3, width = 5, dpi = 450)\n\n#include the age plot in rmd\n#knitr::include_graphics(\"image/eye_plot.png\")\nknitr::include_graphics(\"static_image/eye_plot.png\")\n\n\n\n\n\n\n\n\n\n\nPlot 3: Ternary plot for 3 major attributes comparison\n\n#standarize the clean_player_data for all numeric columns based on min and max value\nplayer_standarized &lt;- clean_player_data %&gt;% \n  mutate(across(where(is.numeric), ~scales::rescale(.x, to = c(0, 1))))\n\n# Plot the ternary plot for three attributes: strength, aggression, and balance\n#Plot the ternary plot for three attributes: strength, aggression, and balance\n\ntern_plot &lt;- ggtern::ggtern(player_standarized, aes(x = finishing, \n                                                    y = dribbling, \n                                                    z = strength, \n                                                    color = player_class)) +\n  geom_point(alpha = 0.3)+\n  theme_bw()+\n  scale_color_manual(values = colors)+\n  # Highlight all points with advanced players\n  geom_point(data = player_standarized %&gt;% \n      filter(player_class == \"Advanced\"), \n              aes(x = finishing, y = dribbling, z = agility))+\n  # Add labels for popular players of presnt data\n  geom_text(data = player_standarized %&gt;% \n            filter(player_name %in% c(\"Lionel Messi\", \"Cristiano Ronaldo\", \"Virgil van Dijk\")), \n              aes(label = player_name),alpha = 1, hjust = -0.1, \n              hjust = 0.5, size = 12, color = \"black\")+\n  # Color the points for Messi and Ronaldo\n  geom_point(data = player_standarized %&gt;% \n            filter(player_name %in% c(\"Lionel Messi\", \"Cristiano Ronaldo\", \"Virgil van Dijk\")), \n                   aes(x = finishing, y = dribbling, z = strength, fill = player_class), \n                   color = 'black', shape = 21, size = 3)+\n  #manual fill color for the highlighted players\n  scale_fill_manual(values = c(\"Advanced\" = \"#7f58AF\", \"Intermediate\" = \"#64C5EB\", \"Novice\" = \"#E84D8A\"))+\n  #remove the legend produced from scale fill manual\n  guides(fill = \"none\")+\n  #increase axis text size\n  theme(axis.text = element_text(size = 36))+\n  theme(axis.title = element_text(size = 52))+\n  theme(axis.title = element_text(family = \"serif\"))+\n  #remove the legend\n  theme(legend.position = \"none\")+\n  geom_confidence_tern(breaks = 0.95)+\n  theme(tern.axis.title.L = element_text(hjust = -.1, vjust = 2),\n        tern.axis.title.R = element_text(hjust = 1, vjust = 2))\n\n\n#save this as jpg using ggsave with name tern_plot\nggsave(plot = tern_plot, filename = \"image/tern_plot.png\", height = 6 , width = 6, dpi = 300)\n\n#Include the ternary plot in the rmd\n#knitr::include_graphics(\"image/tern_plot.png\")\nknitr::include_graphics(\"static_image/tern_plot.png\")\n\n\n\n\n\n\n\n\n\n\nInfographics:aesthetics buildup\nThis segment and beyond involves, creating text, plots, base plots, etc aesthetics for the final infographics. Most of these are derived from UFO plot, as mentioned in the introduction of this document.\n\n#specify text size and fonts\nalien &lt;- c('#47fcea', '#3c6478', '#548687', '#17bd52', '#679d76', '#3e6f50', '#27593d')\ntxt &lt;- alien[7]\nbg &lt;- 'black' # '#010101'\naccent &lt;- txt\n\nsysfonts::font_add(\"fa-brands\", regular = \"data/font/fa-brands-400.ttf\")\nfont_add(\"fa-solid\",  regular = \"data/font/fa-solid-900.ttf\")\nfont_add_google(\"Orbitron\", \"orb\")\nfont_add_google(\"Barlow\", \"bar\")\nshowtext_auto()\nft &lt;- \"orb\"\nft1 &lt;- \"bar\"\n\n# 🔡 text --------------------------------------------------------------------\n\nmastodon &lt;- glue(\"&lt;span style='font-family:fa-brands; color:{accent}'&gt;&#xf4f6;&lt;/span&gt;\")\ntwitter &lt;- glue(\"&lt;span style='font-family:fa-brands; color:{accent}'&gt;&#xf099;&lt;/span&gt;\")\ngithub &lt;- glue(\"&lt;span style='font-family:fa-brands; color:{accent}'&gt;&#xf09b;&lt;/span&gt;\")\nfloppy &lt;- glue(\"&lt;span style='font-family:fa-solid; color:{accent}'&gt;&#xf0c7;&lt;/span&gt;\")\nspace &lt;- glue(\"&lt;span style='color:{bg};font-size:1px'&gt;'&lt;/span&gt;\")\nspace2 &lt;- glue(\"&lt;span style='color:{bg}'&gt;--&lt;/span&gt;\")\ncaption &lt;- glue(\"{mastodon}{space2}@sujan@{space}sujandon.org{space2}{twitter}{space2}@sujan{space2}{github}{space2}sbgithubhm/tidytues{space2}{floppy}{space2}European players attributes comparison\")\n\n\n# Define subtitle with HTML formatting for colors\nsubtitle &lt;- \"&lt;span style='color:#7f58AF'&gt;Advanced players[&gt;85 ratings]&lt;/span&gt;, &lt;span style='color:#64C5EB'&gt;Intermediate players[70-85]&lt;/span&gt;, &lt;span style='color:#E84D8A'&gt;Novice players[&lt;70]&lt;/span&gt;\"\n\n#---------------copy of UFO plot\ng_base &lt;- ggplot() +\n  labs(\n    title = \"European Soccer Players \\n attributes comparison based \\n on Fifa ratings, 2016\",\n    subtitle = subtitle,\n    caption = caption\n    ) +\n  theme_void() +\n  theme(\n    text = element_text(family = ft, size = 36, lineheight = 0.3, colour = txt),\n    plot.background = element_rect(fill = \"white\", colour = bg),\n    plot.title = element_text(size = 128, face = \"bold\", hjust = 0.5, margin = margin(b = 10)),\n    plot.subtitle = element_text(family = ft1, hjust = 0.5, margin = margin(b = 20), color = \"#27593d\", size = 60),\n    plot.caption = element_markdown(family = ft1, colour = colorspace::darken(txt, 0.5), hjust = 0.5,\n                                    margin = margin(t = 20)),\n    plot.margin = margin(b = 20, t = 50, r = 50, l = 50),\n    axis.text.x = element_text())+\n    theme(\n    plot.subtitle = element_markdown()\n    )\n# # quote 1 for the distribution\n\nquote1 &lt;- ggplot() +\n  annotate(\"text\", x = 0, y = 1, label = \"The 'half-eye' plot in the left, illustrates the age \\n distribution of players from the 2016 European soccer \\n database. Advanced players exhibit a relatively \\n concentrated age range, with the lower limit set at \\n 24 years. On the other hand, intermediate and novice  \\n players display flatter distributions. Novice  \\n players, in particular, show a broad age  \\n range, evenly spread from 17 to 45 years. Despite \\n these variations the median age remains similar \\n across all categories\",\n           family = ft1, colour = 'black', size = 16, hjust = 0, fontface = \"italic\", lineheight = 0.4) +\n  xlim(0, 1) +\n  ylim(0, 1) +\n  theme_void() +\n  coord_cartesian(clip = \"off\")\n\n\nquote2 &lt;- ggplot() +\n  annotate(\"text\", x = 0, y = 1, label =\" In comparing overall playing attributes, advanced players \\n show slightly higher finish (ability to score goal),\\n crossing (skill to pass the ball to team), \\n and dribbling (ability to pass through opponent).\\n Aggression, strenth, and balance are similar \\n among all players. However, the difference is subtle,\\n off by only few percentage\",\n           family = ft1, colour = 'black', size = 16, hjust = 0, fontface = \"italic\", lineheight = 0.4) +\n  xlim(0, 1) +\n  ylim(0, 1) +\n  theme_void() +\n  coord_cartesian(clip = \"off\")\n\n#quote 3\nquote3 &lt;- ggplot() +\n  annotate(\"text\", x = 0, y = 1, label = \"The comparison of three most critical attributes \\n     finishing, strength, and dribbling—a high \\n         similarity is observed between intermediate and \\n            novice players. However, advanced players exhibit \\n                a narrower overlap. This suggests distinctions \\n                      between players of varying FIFA ratings are subtle.\",\n                    family = ft1, colour = 'black', size = 16, hjust = 0, fontface = \"italic\", lineheight = 0.4) +\n  xlim(0, 1) +\n  ylim(0, 1) +\n  theme_void() +\n  coord_cartesian(clip = \"off\")\n                                                  \nquote4 &lt;- ggplot()+\nannotate(\"text\", x= 0, y = 1, label = \"Lionel Messi, the world's best player,\\n     does not stand out, other players demonstrate \\n          similar capabilities. Given that the outcome in \\n                sports game relies heavily on overall team \\n                    performance, individual attributes \\n                         measures can be wrong metrics for\\n                               game winning\",\n  \n           family = ft1, colour = \"black\", size = 16, hjust = 0, fontface = \"italic\", lineheight = 0.4) +\n  xlim(0, 1) +\n  ylim(0, 1) +\n  theme_void() +\n  coord_cartesian(clip = \"off\")\n\nImages cannot be loaded into ggplot object directly. it should be convereted to raster object and then it can be overlaid over other ggplot object. The following codes achieves the same thing for different images that are used in the final infographics.\n\n#load the tern image\nimage &lt;- readPNG(\"image/tern_plot.png\")\ntern_image &lt;- as.raster(image)\n\n#radar plot\nradar &lt;- readPNG(\"image/radar_plot.png\")\nradar_image &lt;- as.raster(radar)\n\n#tern plot image\nimage &lt;- readPNG(\"image/tern_plot.png\")\nimage &lt;- as.raster(image)\n\n#lionel messi image\nmessi &lt;- readJPEG(\"data/images/messi.jpg\")\nmessi &lt;- as.raster(messi)\n\n#fifa logo image\nfifa &lt;- readPNG(\"data/images/FIFA.png\")\nfifa &lt;- as.raster(fifa)\n\n#goalpost as image\nsoccer_field &lt;- readJPEG(\"data/images/soccer-field.jpg\")\nsoccer_field &lt;- as.raster(soccer_field)\n\nThe final graph is produced from the following line. This chunk adds all previously made plots and inset them into appropriate locations, based on preference. This process takes really long time, as finding the exact location from just point values is significantly tedious task. I would suggest using any other graphical interface software to add the quotes and paragraphs, which takes way less time.\n\n# Combine the plots into a single infographic\ng_final &lt;- g_base +\n  inset_element(fifa, left = 0.9, right = 1.08, top = 0.25, bottom = -0.2)+\n  inset_element(eye_plot, left = -0.12, right = 0.57, top = 1, bottom = 0.66) +\n  #insert radar plot\n  inset_element(tern_image, left = -0.10, right = 0.80, top = 0.5, bottom = -.1) +\n  inset_element(messi, left = -0.08, right = 0.15, top = 0.50, bottom = 0.15) +\n  inset_element(radar_plot, left = 0.4, right = 1.1, top = 0.81, bottom = 0.31) +\n  #insert quote 1\n  inset_element(quote1, left = 0.5, right = 1, top = 0.88, bottom = 0.72) +\n  #Insert quote 2\n  inset_element(quote2, left = -0.07, right = 0.5, top = 0.55, bottom = 0.5) +\n  #insert quote 3\n  inset_element(quote3, left = 0.41, right = 1, top = 0.31, bottom = 0) +\n  #insert quote 4\n  inset_element(quote4, left = 0.54, right = 1, top = 0.13, bottom = -0.10)+\n  plot_annotation(theme = theme(\n      plot.background = element_rect(fill = \"white\", colour = 'white')))\n\nggsave(plot = g_final, filename = \"infographics_draft.png\", height = 16, width = 10)"
  },
  {
    "objectID": "featured_projects/econometrics/index.html",
    "href": "featured_projects/econometrics/index.html",
    "title": "Did Smoking Mothers Have Lowered Weight Children?",
    "section": "",
    "text": "Summary and Conclusion\nThe analysis aimed to estimate the causal effect of maternal smoking during pregnancy on infant birth weight. Initial comparisons showed a significant difference in birth weights between infants of smoking and non-smoking mothers. However, confounding variables suggested that the groups were not comparable.\nPropensity score matching was applied to address these confounders. Post-matching tests indicated successful balancing of the treatment and control groups. The final analysis showed that maternal smoking causally reduces infant birth weight by an average of 13 grams, after accounting for confounding variables. This result highlights the importance of proper matching techniques in observational studies to draw valid causal inferences\n\n\nProject Begins:\nThe goal is to estimate the causal effect of maternal smoking during pregnancy on infant birth weight using the treatment ignorability assumptions. The data are taken from the National Natality Detail Files, and the extract “SMOKING_EDS241.csv”’ is a random sample of all births in Pennsylvania during 1989-1991. Each observation is a mother-infant pair. The key variables are:\nThe outcome and treatment variables are:\nbirthwgt=birth weight of infant in grams\ntobacco=indicator for maternal smoking\nThe control variables are:\nmage (mother’s age), meduc (mother’s education), mblack (=1 if mother identifies as Black), alcohol (=1 if consumed alcohol during pregnancy), first (=1 if first child), diabete (=1 if mother diabetic), anemia (=1 if mother anemic)\n\nlibrary(here)\nlibrary(tidyverse)\nlibrary(knitr)\nlibrary(stargazer)\nlibrary(plm)\nlibrary(pglm)\nlibrary(dplyr)\nlibrary(MatchIt)\nlibrary(lmtest)\nlibrary(RItools)\nlibrary(sandwich)\nlibrary(estimatr)\nlibrary(Hmisc)\n\n\n# Load data\nsmoking_data &lt;- read_csv(here(\"featured_projects/econometrics/data/SMOKING_EDS241.csv\"))\n\n\nMean Differences, Assumptions, and Covariates\nFor conducting t-test, it is important to have population divided into treatment and control group. Since this experiment already has that information, I can segregate them into two tables and perform t-tests.\n\n## calculating difference using t.test when tobacco is 1 and 0\nsmoking_mothers = smoking_data %&gt;% filter(tobacco == 1)\nnon_smoking_mothers = smoking_data %&gt;% filter(tobacco == 0)\n\n#peform t-test to see if the difference is significant in other covariates\nt.test(smoking_mothers$birthwgt, non_smoking_mothers$birthwgt)\n\n\n    Welch Two Sample t-test\n\ndata:  smoking_mothers$birthwgt and non_smoking_mothers$birthwgt\nt = -58.932, df = 26945, p-value &lt; 2.2e-16\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -252.6727 -236.4060\nsample estimates:\nmean of x mean of y \n 3185.747  3430.286 \n\n\nThe mean difference is 244.539 grams between children from smoker and non-smoker mothers, which is significant at 5%. The assumptions for this mean difference to hold are ignorability (no other confounding variables influencing the outcome) and common support (there is sufficient overlap between the treatment and control group). If these assumptions are satisfied, I can infer the difference is actually due to smoking and not due to random chance.\nI can test those assumptions with t-tests for numeric covariates, and proportion test for categorical covariates The following code achives that:\n\n#set options to have maximum 5 decimal\noptions(digits=5)\n## For continuous variables I can use the t-test\n#t.test()\neducation &lt;- t.test(smoking_mothers$meduc, non_smoking_mothers$meduc)\nage &lt;- t.test(smoking_mothers$mage, non_smoking_mothers$mage)\nbirthwht &lt;- t.test(smoking_mothers$birthwgt, non_smoking_mothers$birthwgt)\n\n## For binary variables I should use the proportions test\n#prop.test()\nalcohol &lt;- prop.test(table(smoking_mothers$alcohol), table(non_smoking_mothers$alcohol))\nfirst_child &lt;-prop.test(table(smoking_mothers$first), table(non_smoking_mothers$first))\ndiabetes&lt;- prop.test(table(smoking_mothers$diabete), table(non_smoking_mothers$diabete))\nanaemia &lt;- prop.test(table(smoking_mothers$anemia),  table(non_smoking_mothers$anemia))\nblack   &lt;- prop.test(table(smoking_mothers$mblack),  table(non_smoking_mothers$mblack))\n\n#-------------- Covariate Calculations and Tables\n\n# create dataframe of coefficents from above results including\n# first column should be variable name, then mean of estimate for sample 1, then\n# mean of sample 2, then p values \ntable &lt;- data.frame(\n  variable = c(\"birthweight\", \"education\", \"age\", \"alcohol\", \"first_child\", \"diabetes\", \"anaemia\", \"black\"),\n  smoking_mothers = c(birthwht$estimate[1], education$estimate[1], age$estimate[1], \n                      sum(smoking_mothers$alcohol)/length(smoking_mothers$alcohol),  \n                      sum(smoking_mothers$first)/length(smoking_mothers$first), \n                      sum(smoking_mothers$diabete)/length(smoking_mothers$diabete), \n                      sum(smoking_mothers$anemia)/length(smoking_mothers$anemia), \n                      sum(smoking_mothers$mblack)/length(smoking_mothers$mblack)),\n  \n  non_smoking_mothers = c(birthwht$estimate[2], education$estimate[2], age$estimate[2], \n                          sum(non_smoking_mothers$alcohol)/length(non_smoking_mothers$alcohol),\n                          sum(non_smoking_mothers$first)/length(non_smoking_mothers$first),\n                          sum(non_smoking_mothers$diabete)/length(non_smoking_mothers$diabete), \n                          sum(non_smoking_mothers$anemia)/length(non_smoking_mothers$anemia), \n                          sum(non_smoking_mothers$mblack)/length(non_smoking_mothers$mblack)),\n  \n  p_value = round(c(birthwht$p.value, \n                    education$p.value, \n                    age$p.value, \n                    alcohol$p.value, \n                    first_child$p.value, \n                    diabetes$p.value, \n                    anaemia$p.value, \n                    black$p.value), 6))\n\nprint(table)\n\n     variable smoking_mothers non_smoking_mothers p_value\n1 birthweight      3.1857e+03          3.4303e+03       0\n2   education      1.1921e+01          1.3239e+01       0\n3         age      2.5539e+01          2.7453e+01       0\n4     alcohol      4.4182e-02          7.1033e-03       0\n5 first_child      3.6459e-01          4.3609e-01       0\n6    diabetes      1.7519e-02          1.7364e-02       0\n7     anaemia      1.4103e-02          7.8005e-03       0\n8       black      1.3541e-01          1.0863e-01       0\n\n\nInterpretation: The result shows that the population sample on treated and control group is dissimilar. The p values are less than .05, which means alternative hypothesis shoule be accepted. Alternative hypothesis for this test is: there is difference between treated and control group.\nSo far, I know there is already a difference in the two samples. But we can still quantify how much these covariates influence these biased samples. The following chunk will do that by calculating average treatment effects with a regression model for these biased groups.\n\n# ATE Regression univariate\ntobacco_univariate &lt;- lm(birthwgt ~ tobacco, data = smoking_data)\n\n# ATE with covariates\ntobacco_covariates &lt;- lm(birthwgt ~  tobacco + mage +  meduc +\n                           mblack + alcohol + first + diabete + anemia, \n                      data = smoking_data)\n\n## create combined table\nstargazer(tobacco_univariate, tobacco_covariates, type = \"text\", \n          out.header = TRUE, \n          title = \"Regression with and without controls\",\n          notes.label = \"significance level\")\n\n\nRegression with and without controls\n===========================================================================\n                                      Dependent variable:                  \n                    -------------------------------------------------------\n                                           birthwgt                        \n                                (1)                         (2)            \n---------------------------------------------------------------------------\ntobacco                     -244.540***                 -228.070***        \n                              (4.079)                     (4.177)          \n                                                                           \nmage                                                      -0.694*          \n                                                          (0.357)          \n                                                                           \nmeduc                                                    11.688***         \n                                                          (0.860)          \n                                                                           \nmblack                                                  -240.030***        \n                                                          (5.106)          \n                                                                           \nalcohol                                                  -77.350***        \n                                                          (13.465)         \n                                                                           \nfirst                                                    -96.944***        \n                                                          (3.447)          \n                                                                           \ndiabete                                                  73.228***         \n                                                          (12.104)         \n                                                                           \nanemia                                                     -4.796          \n                                                          (16.754)         \n                                                                           \nConstant                    3,430.300***                3,362.300***       \n                              (1.791)                     (11.927)         \n                                                                           \n---------------------------------------------------------------------------\nObservations                   94,173                      94,173          \nR2                             0.037                       0.072           \nAdjusted R2                    0.037                       0.072           \nResidual Std. Error     493.750 (df = 94171)        484.730 (df = 94164)   \nF Statistic         3,594.300*** (df = 1; 94171) 909.180*** (df = 8; 94164)\n===========================================================================\nsignificance level                              *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01\n\n\nIf I have to see the result above, seems like all covariates are responsible for change in birthweights in the smoking and non smoking mothers. But since the treated and control groups are dissimilar, it should not be inferred that these covariates are producing change.\nI can do one more test before I process the data to make them true. I can test if any of the covariates have a similar population between the treated and control groups already. I can use chi-squared tests among all these variables to see which one truly represents a similar population being compared.\n\n# perform balance test\nx &lt;- xBalance(tobacco ~ mage +  meduc + mblack + alcohol + first + diabete + anemia + birthwgt,  data = smoking_data,\n         report=c(\"std.diffs\",\"chisquare.test\", \"p.values\"))\n\n#use staragazer to present the results\nas.data.frame(x[1]) %&gt;% \n   #round last column to 5 digit\n  mutate_if(is.numeric, round, 5) %&gt;% \n  #rename columns based on number index\n  setNames(c( \"chi-Square/standard difference test\", \"p-value\"))\n\n         chi-Square/standard difference test p-value\nmage                                -0.36194  0.0000\nmeduc                               -0.64374  0.0000\nmblack                               0.08439  0.0000\nalcohol                              0.31525  0.0000\nfirst                               -0.14500  0.0000\ndiabete                              0.00119  0.8858\nanemia                               0.06670  0.0000\nbirthwgt                            -0.49527  0.0000\n\n\nOnly the diabetic covariate is similar between sample group at this point. What this means is that the population samples between treatment and control group are similar only in regards to diabetes.\nLets also calculate propensity estimation for this biased sample:\n\n\nPropensity Score Estimation for the biased Treated and control Groups\n\n## Propensity Scores estimation with logistic regression\nmother_prospensityscore &lt;- glm(tobacco ~  mage +  meduc +\n                           mblack + alcohol + first + diabete + anemia + birthwgt, data = smoking_data,\n                              family = binomial())\n\n## create a table of coefficients\nstargazer:: stargazer(mother_prospensityscore, type = \"text\")\n\n\n=============================================\n                      Dependent variable:    \n                  ---------------------------\n                            tobacco          \n---------------------------------------------\nmage                       -0.040***         \n                            (0.002)          \n                                             \nmeduc                      -0.288***         \n                            (0.005)          \n                                             \nmblack                     -0.361***         \n                            (0.028)          \n                                             \nalcohol                    1.962***          \n                            (0.062)          \n                                             \nfirst                      -0.460***         \n                            (0.020)          \n                                             \ndiabete                    0.229***          \n                            (0.067)          \n                                             \nanemia                     0.333***          \n                            (0.081)          \n                                             \nbirthwgt                   -0.001***         \n                           (0.00002)         \n                                             \nConstant                   6.456***          \n                            (0.090)          \n                                             \n---------------------------------------------\nObservations                94,173           \nLog Likelihood            -40,841.000        \nAkaike Inf. Crit.         81,699.000         \n=============================================\nNote:             *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01\n\n#create regression table dataframe based on mother_prospensityscore\n# Assuming you have a regression model object named 'model'\n# You would need to replace 'model' with the actual name of your model object\n\n\nmodel = mother_prospensityscore\n# Extract coefficients, standard errors, and p-values from the model\ncoefficients &lt;- coef(model)\nstandard_errors &lt;- sqrt(diag(vcov(model)))\np_values &lt;- summary(model)$coefficients[, 4]  # Extracting p-values\n\n# Define function to determine significance level\nget_significance_level &lt;- function(p_value) {\n  if (p_value &lt; 0.01) {\n    return('***')\n  } else if (p_value &lt; 0.05) {\n    return('**')\n  } else if (p_value &lt; 0.1) {\n    return('*')\n  } else {\n    return('')\n  }\n}\n\n# Get significance levels\nsignificance_levels &lt;- sapply(p_values, get_significance_level)\n\n# Create dataframe\ndf &lt;- data.frame(\n  Coefficient = coefficients,\n  Standard_error = round(standard_errors, 3),\n  Significance_level = significance_levels\n)\n\n# Print the dataframe\nprint(df)\n\n            Coefficient Standard_error Significance_level\n(Intercept)  6.45614093          0.090                ***\nmage        -0.04012754          0.002                ***\nmeduc       -0.28772570          0.005                ***\nmblack      -0.36100043          0.028                ***\nalcohol      1.96216177          0.062                ***\nfirst       -0.45972476          0.020                ***\ndiabete      0.22927944          0.067                ***\nanemia       0.33284394          0.081                ***\nbirthwgt    -0.00091614          0.000                ***\n\n\nThe covariates coefficients are strengths/weights of each variables that determines whether a unit will recieve the treatment. For example, the coefficient of mother’s age is 0.04, which means that for every unit increase in mother’s age, keeping all other covariates constant, the log odds of being in treatment group decreases by 0.02. This is statistically significant as evident by p value, which is much less than 0.05. All covariates in the model are significant which means they are all important in determining the treatment status for a unit X.\nAmong all covariates, alcohol has the greatest influence on whether a unit will recieve treatment, followed by first born child, which negative influence in being treatment group. These coefficients are all significant at 5% significance level.\nIn summary: what the above table is showing is that: all these covariates are responsible whether a unit falls into a treatment or control group. This is bad because an experiment should be completely random and should not influenced by anything. that means significance level for all those should have been more than 0.05.\nLook at the side by side histogram below. That is asymmetric. It should be made symmetric. and then I can run all analysis again .\n\n## use this logistic model to create a new column of prospensity scores for each observation\nsmoking_data$prospensity_scores &lt;- predict(mother_prospensityscore, type = \"response\")\n\n#round the scores to 2 decimal places\nsmoking_data$prospensity_scores &lt;- round(smoking_data$prospensity_scores, 2)\n\n## Histogram of PS before matching\nhistbackback(split(smoking_data$prospensity_scores, smoking_data$tobacco),\n             main= \"Propensity score before matching\",  \n             xlab=c(\"control\",  \"treatment\"))\n\n\n\n\n\n\n\n\nOverlap and its meaning Overlap refers to the degree of similarity or commonality between the treatment group (those who received the treatment being studied) and the control group (those who did not receive the treatment). The overlap is important because it is a necessary condition for the ignorability assumption to hold. If there is no overlap, then the treatment and control groups are so different that it is impossible to make causal inferences. The histogram above shows that there is a only some overlap between the treatment and control group, more individuals in the control group with lower propensity scores than in the treatment group. This is a violation of the Common Support assumption.\nBased on the analysis done so far. So, to resolve that I can select only those subjects that can create similarity in the samples.\nThat will involve following steps:\n-Calculate propensity score and match based on propensity values, then rerun all analysis as before. Propensity score calculates the probability that a unit will fall into treatment group.\n**Okay: lets begin the real analysis on unbiased data: The steps are:\n\nmatch the population and rerun ATE\nrerun average treatment effect on treated only\nperform more analysis matching samples by neighbour method, inverse weighted method\n\n\n\nSTEP 1. Matching the population with propensity score\nMatch treated/control mothers using your estimated propensity scores and nearest neighbor matching. Compare the balancing of pretreatment characteristics (covariates) between treated and non-treated units in the original dataset.\n\n## Nearest-neighbor Matching\nprospensity_score_matched &lt;- MatchIt::matchit(tobacco ~  mage +  meduc +\n                             mblack + alcohol + first + diabete + anemia + birthwgt, data = smoking_data, method = \"nearest\")\n\n## Covariate Imbalance post matching\nmatched_prospensity_dataset &lt;- match.data(prospensity_score_matched)\n\n# Drawing back to back histograms for propensity scores for treated and \n# non-treated after matching\nhistbackback(split(matched_prospensity_dataset$prospensity_scores,  matched_prospensity_dataset$tobacco),   main= \"Propensity\n        score   after   matching\",  xlab=c(\"control\",   \"treatment\"))\n\n\n\n\n\n\n\n\nPost matching, there is a better overlap between the treatment and control group. Units with high propensity score is matched with its counterparts and vice versa. This means the units we are comparing between the treatment and control group are similar. This helps us define counterfactuals of what would have happened to the treatment group if they were not treated.\n\n# the covariates between treated and non-treated that were used in the\n# estimation of the propensity scores\nxBalance(tobacco ~ mage +  meduc + mblack + alcohol + first + diabete + anemia + birthwgt, data = matched_prospensity_dataset,\n         report=c(\"std.diffs\",\"chisquare.test\", \"p.values\"))\n\n         strata():  unstrat    \n         stat      std.diff    \nvars                           \nmage                    0.0 ***\nmeduc                   0.0 ***\nmblack                  0.0 ** \nalcohol                 0.1 ***\nfirst                   0.0 ***\ndiabete                 0.0 .  \nanemia                  0.0    \nbirthwgt                0.0 *  \n---Overall Test---\n        chisquare df p.value\nunstrat       175  8   1e-33\n---\nSignif. codes:  0 '***' 0.001 '** ' 0.01 '*  ' 0.05 '.  ' 0.1 '   ' 1 \n\nxBalance(tobacco ~ mage +  meduc + mblack + alcohol + first + diabete + anemia + birthwgt, data = smoking_data,\n         report=c(\"std.diffs\",\"chisquare.test\", \"p.values\"))\n\n         strata():  unstrat     \n         stat      std.diff     \nvars                            \nmage                   -0.4 *** \nmeduc                  -0.6 *** \nmblack                  0.1 *** \nalcohol                 0.3 *** \nfirst                  -0.1 *** \ndiabete                 0.0     \nanemia                  0.1 *** \nbirthwgt               -0.5 *** \n---Overall Test---\n        chisquare df p.value\nunstrat     10298  8       0\n---\nSignif. codes:  0 '***' 0.001 '** ' 0.01 '*  ' 0.05 '.  ' 0.1 '   ' 1 \n\n\nAfter the matching, the nature and weight of the regression coefficients changed. Previously in unmatched data, I discussed that with increase age of mother, propensity score decreases. But post matching, the coefficients is showing that propensity score actually increases with increasing age of the mother. This is a sign that matching have accounted fixed effects in the observational dataset. Moreover, some covariates which were significant in determining the treatement status in the pre matching, are not significant anymore post matching. Like diabete, anemia, birthwgt, and mblack. Thus mother being black, having diabetes, and having anaemia, does not determine if she will receive the treatment or not.\nBut, this conclusion is not yet sufficient. What if I see this change only on treated population and not in control group ? The follwing step involves that:\n\n\nSTEP 2: Average treatment effect on treated group\nThis step is necessary because it can be more robust estimate. For example: this will compare the change before treatment and after treatment in the same units.\n\n## calculate ATE based on nearest neighbor matching\nsumdiff_data &lt;- matched_prospensity_dataset%&gt;%\n  group_by(subclass)%&gt;%\n  mutate(diff = birthwgt[tobacco==1]- birthwgt[tobacco==0])\n\ndif_in_treated &lt;- sum(sumdiff_data$diff)/2\n\nATT_weighted_count = 1/sum(matched_prospensity_dataset$tobacco) * dif_in_treated\nATT_weighted_count\n\n[1] -13.361\n\n\nFor the treated smoker mothers, the tobacco effects on their child birth weight is lower by 13 grams on average than for its counterfactual non smoking mothers. This means that the treatment has caused lower birth weight in the treated group of mothers. For any other units of population who shares similar covariates as treated group, the birth weight of their child will be lower by 13 grams on average if they were to start smoking tobacco.\nIs the conclusion final ? not yet. What if the propensity matching has drawbacks, which it does already. Propensity score matching is based on the fact that all covariates have similar influence in treatment. But, this does not always hold true. WLS matching fixes that.\n\n\nATE with WLS Matching\nThis step is necessary becuase the\n\n## Weighted least Squares (WLS) estimator Preparation\nsmoking_data &lt;- smoking_data %&gt;% \n  mutate(weights = tobacco / prospensity_scores + (1 - tobacco) / (1 - prospensity_scores))\n\n## Weighted least Squares (WLS) Estimates\nwls &lt;- lm(birthwgt ~ tobacco + mage +  meduc +\n                           mblack + alcohol + first + diabete + anemia, \n          data = smoking_data, weights = weights)\n\nsummary(wls)\n\n\nCall:\nlm(formula = birthwgt ~ tobacco + mage + meduc + mblack + alcohol + \n    first + diabete + anemia, data = smoking_data, weights = weights)\n\nWeighted Residuals:\n   Min     1Q Median     3Q    Max \n -7283   -374     32    396  12243 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 3122.734     11.584  269.58  &lt; 2e-16 ***\ntobacco        1.426      3.241    0.44     0.66    \nmage           0.219      0.358    0.61     0.54    \nmeduc         23.497      0.881   26.69  &lt; 2e-16 ***\nmblack      -215.695      4.978  -43.33  &lt; 2e-16 ***\nalcohol     -174.499     13.803  -12.64  &lt; 2e-16 ***\nfirst        -65.326      3.509  -18.61  &lt; 2e-16 ***\ndiabete       72.223     12.213    5.91  3.4e-09 ***\nanemia       -23.746     16.766   -1.42     0.16    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 698 on 94163 degrees of freedom\n  (1 observation deleted due to missingness)\nMultiple R-squared:  0.0369,    Adjusted R-squared:  0.0368 \nF-statistic:  451 on 8 and 94163 DF,  p-value: &lt;2e-16\n\n## Present results\n\nThe WLS matching is weighted based on propensity scores, meaning unit with higher similarity in covariates gets more weights. Based on this matching, the average birth weight of children for non smoker mother or control group is 3122.7338, keeping all other variables unchanged. This is statistically significant at 5% signficance. Of all the covariates, mblack covariates has greatest influence in outcome. i.e If the women identifies as black, the birth weight of the child is lower by 241 grams, which is significant. Besides, other covariates like drinking alcohol, first born child, and diabetes in covariates also significantly influence birthweight. However, tobacco appears insignificant at 5 percent signficance level. The model only explains 3 percent of variation given by R squared in birth weight of children. This means that the model is not a good fit for the data.\nThere are certain factors that influences the output more than other covariates. This was also shown by the coefficients in Balance estimates in previous step. When using ATT with propensity score matching, the model assumed that all covariates has equal influence in determining the treatment status. But in reality, this is not always true. The WLS matching takes weights of each covariates into account, thus providing different output than ATT. if all the covariates had same weights, we would have got same estimate using both ATT and WLS matching.\nIs the conclusion final ?\n-Statistically YES !"
  },
  {
    "objectID": "featured_projects/calcofi/index.html",
    "href": "featured_projects/calcofi/index.html",
    "title": "Using AI for Good: Helping non profits in predicting carbon calculation",
    "section": "",
    "text": "XYZ, name hidden for security purpose, was established in 1910 to investigate the ecological factors contributing to the collapse of the Pacific sardine population. Its extensive time series data provides invaluable insights into the long-term impacts of environmental changes on marine ecosystems and the communities reliant on them, not only within the California Current System but also extending to the North Pacific and beyond on an international scale.\nAs a part of this project, I am supporting the XYZ in predicting the ocean carbon values, which is a crucial part of the marine ecosystem. The dataset contains 12 columns, with 1 response variable and 11 predictors. The response variable is the dissolved inorganic carbon (DIC) concentration in the ocean, which is a key component of the marine carbon cycle. The predictors include various physical and chemical properties of the ocean, such as temperature, salinity, and oxygen concentration.\nThe goal of this project is to develop two machine learning models that can accurately predict the DIC concentration in the ocean based on the given predictors. This model will help better understand the factors influencing ocean carbon levels and make data-driven decisions to protect marine ecosystems.\n\nStepwise Flow of the Project\n\nLinear Regression Model\nFine Tuned XGBoost Model\n\n\n#hide all warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n#### Import all necessary libraries\n# import all required libraries\nimport numpy as np\nimport xgboost as xgb\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import cross_val_score, KFold\nfrom sklearn.linear_model import SGDRegressor\nfrom sklearn.linear_model import Ridge\nfrom sklearn.preprocessing import PolynomialFeatures\n\nRead the data and store ID:required during submission for validity check\n\n#-----read train and test set\ntrain_calcofi = pd.read_csv(\"data/train.csv\")\ntest_calcofi  = pd.read_csv(\"data/test.csv\")  \n\n#collect the test ids for testcalcofi, required for submission dataset\ntest_ids = test_calcofi.id\ntest_calcofi = test_calcofi.drop(columns=['id'])\n\n\nData cleaning\nA model is only as good as the data.This step ensures that columns names are standarized, and the columns with inappropriate values are removed. The data is also checked for missing values. Incase of missing values, they are not imputed, but dropped. The CALCOFI did not provide claer guidance on how value should be imputted. So the best decision is to drop the rows with missing values.\n\n#----inspect the head and take the insights of data\ntrain_calcofi.head()\n\n   id    Lat_Dec     Lon_Dec  ...  Salinity1  Temperature_degC      DIC\n0   1  34.385030 -120.665530  ...     34.198              7.82  2270.17\n1   2  31.418333 -121.998333  ...     34.074              7.15  2254.10\n2   3  34.385030 -120.665530  ...     33.537             11.68  2111.04\n3   4  33.482580 -122.533070  ...     34.048              8.36  2223.41\n4   5  31.414320 -121.997670  ...     34.117              7.57  2252.62\n\n[5 rows x 19 columns]\n\n#### Data cleaning and preprocessing\n#the column names are in snake case, change all to lowercase\ntrain_calcofi.columns = map(str.lower, train_calcofi.columns)\ntest_calcofi.columns = map(str.lower, test_calcofi.columns)\n\n\n#remove the unnamed:12 column\ntrain_calcofi = train_calcofi.drop(columns=['unnamed: 12'])\ntrain_calcofi.rename(columns={'ta1.x': 'ta1'}, inplace=True)\n\nThe data looks clean. Now, a relationships between columns must be established This helps in understanding the data, and also helps in feature selection. The next step below plots a correlation matrix. This will show correlated variables in the dataset.\nThe reason that the correlation matrix is plotted to see if linear regression can be useful. If the correlation matrix shows strong relationship between the response and predictors, then linear regression is a great algorithm. If not, then other models must be tested.\n\n#plot correlation matrix\ncorr_matrix = train_calcofi.corr()\n\n# Plot the heatmap\nplt.figure(figsize=(8, 6))\nsns.heatmap(corr_matrix, annot=False, cmap='coolwarm', fmt=\".1f\")\nplt.title('Correlation Heatmap')\nplt.show()\n\n\n\n\n\n\n\n\n\nLinear regression model\n\n# Select only the predictors columns, and change them to array\nX = train_calcofi.drop(columns=['dic', 'id'], axis=1)\n\n# Select only the response column and change it to array\ny = train_calcofi['dic']\n\n# Split the data into training, validation, and test sets\nX_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\nX_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n\n# Define the range of polynomial degrees to fit\ndegrees = range(1, 5)  # From 1 to 5\n\n# Initialize lists to store R^2 scores\ntrain_r2_scores = []\nval_r2_scores = []\n\n# Loop through each polynomial degree\nfor degree in degrees:\n    # Create a pipeline with PolynomialFeatures, StandardScaler, and LinearRegression\n    model_pipeline = make_pipeline(PolynomialFeatures(degree=degree), StandardScaler(), LinearRegression())\n\n    # Fit the model pipeline to the training data\n    model_pipeline.fit(X_train, y_train)\n\n    # Calculate R^2 on the training set\n    train_r2 = model_pipeline.score(X_train, y_train)\n    train_r2_scores.append(train_r2)\n\n    # Calculate R^2 on the validation set\n    val_r2 = model_pipeline.score(X_val, y_val)\n    val_r2_scores.append(val_r2)\n\n    # Print the results for each degree\n    print(f\"Degree: {degree}\")\n    print(f\"  R^2 on training set: {train_r2}\")\n    print(f\"  R^2 on validation set: {val_r2}\")\n    print(\"-\" * 40)\n\nPipeline(steps=[('polynomialfeatures', PolynomialFeatures(degree=4)),\n                ('standardscaler', StandardScaler()),\n                ('linearregression', LinearRegression())])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  Pipeline?Documentation for PipelineiFittedPipeline(steps=[('polynomialfeatures', PolynomialFeatures(degree=4)),\n                ('standardscaler', StandardScaler()),\n                ('linearregression', LinearRegression())])  PolynomialFeatures?Documentation for PolynomialFeaturesPolynomialFeatures(degree=4)  StandardScaler?Documentation for StandardScalerStandardScaler()  LinearRegression?Documentation for LinearRegressionLinearRegression() \n\n# Plotting the R^2 scores for training and validation sets\nplt.figure(figsize=(10, 6))\nplt.plot(degrees, train_r2_scores, label='Training R^2', marker='o')\nplt.plot(degrees, val_r2_scores, label='Validation R^2', marker='o')\nplt.xlabel('Polynomial Degree')\nplt.ylabel('R^2 Score')\nplt.title('Polynomial Degree vs. R^2 Score')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n#from the above sample, it is clear that either degree 1 or 2 is best\n# Define the polynomial degree\ndegree = 1\n\n# Define the range of regularization parameters (alpha values) to test\nalphas = np.logspace(-3, 3,  10)  # e.g., 10^-4 to 10^4\n\n# Initialize lists to store R^2 scores\ntrain_r2_scores = []\nval_r2_scores = []\n\n# Loop through each alpha value\nfor alpha in alphas:\n    # Create a pipeline with PolynomialFeatures, StandardScaler, and Ridge regression\n    model_pipeline = make_pipeline(PolynomialFeatures(degree=degree), StandardScaler(), Ridge(alpha=alpha))\n\n    # Fit the model pipeline to the training data\n    model_pipeline.fit(X_train, y_train)\n\n    # Calculate R^2 on the training set\n    train_r2 = model_pipeline.score(X_train, y_train)\n    train_r2_scores.append(train_r2)\n\n    # Calculate R^2 on the validation set\n    val_r2 = model_pipeline.score(X_val, y_val)\n    val_r2_scores.append(val_r2)\n\n    # Print the results for each alpha\n    print(f\"Alpha: {alpha}\")\n    print(f\"  R^2 on training set: {train_r2}\")\n    print(f\"  R^2 on validation set: {val_r2}\")\n    print(\"-\" * 40)\n\nPipeline(steps=[('polynomialfeatures', PolynomialFeatures(degree=1)),\n                ('standardscaler', StandardScaler()),\n                ('ridge', Ridge(alpha=np.float64(1000.0)))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  Pipeline?Documentation for PipelineiFittedPipeline(steps=[('polynomialfeatures', PolynomialFeatures(degree=1)),\n                ('standardscaler', StandardScaler()),\n                ('ridge', Ridge(alpha=np.float64(1000.0)))])  PolynomialFeatures?Documentation for PolynomialFeaturesPolynomialFeatures(degree=1)  StandardScaler?Documentation for StandardScalerStandardScaler()  Ridge?Documentation for RidgeRidge(alpha=np.float64(1000.0)) \n\n# Plotting the R^2 scores for training and validation sets\nplt.figure(figsize=(12, 6))\nplt.plot(alphas, train_r2_scores, label='Training R^2', marker='o', linestyle='-', color='b')\nplt.plot(alphas, val_r2_scores, label='Validation R^2', marker='o', linestyle='-', color='r')\nplt.xscale('log')  # Log scale for alpha values\nplt.xlabel('Regularization Parameter (Alpha)')\nplt.ylabel('R^2 Score')\nplt.title('Regularization Parameter (Alpha) vs. R^2 Score')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n#finalize the model\n# Split data into training and validation sets\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Define the polynomial degree and regularization parameter (lambda)\ndegree = 1\nalpha = 50  # Regularization parameter\n\n# Create and fit the model pipeline\nmodel_pipeline = make_pipeline(\n    PolynomialFeatures(degree=degree),\n    StandardScaler(),\n    Ridge(alpha=alpha)\n)\n\n# Fit the model to the training data\nmodel_pipeline.fit(X_train, y_train)\n\nPipeline(steps=[('polynomialfeatures', PolynomialFeatures(degree=1)),\n                ('standardscaler', StandardScaler()),\n                ('ridge', Ridge(alpha=50))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  Pipeline?Documentation for PipelineiFittedPipeline(steps=[('polynomialfeatures', PolynomialFeatures(degree=1)),\n                ('standardscaler', StandardScaler()),\n                ('ridge', Ridge(alpha=50))])  PolynomialFeatures?Documentation for PolynomialFeaturesPolynomialFeatures(degree=1)  StandardScaler?Documentation for StandardScalerStandardScaler()  Ridge?Documentation for RidgeRidge(alpha=50) \n\n# Evaluate the model\ntrain_r2 = model_pipeline.score(X_train, y_train)\nval_r2 = model_pipeline.score(X_val, y_val)\n\nprint(f\"Training R^2 score for Linear regression: {train_r2}\")\n\nTraining R^2 score for Linear regression: 0.9964417610035372\n\nprint(f\"Validation R^2 score for linear regression: {val_r2}\")\n\nValidation R^2 score for linear regression: 0.9964454374974873\n\n\nThe base linear regression model has worked well with 1 degree polynomial and low regularization parameters, with mean squared error of 36 on testing set, meaning ocean carbon values(DIC) was off by 36 point on average for the prediction test.\n\n\n\nCan XGBoost perform better ?\nThe next step involves using XGboost for making the prediction, Extreme Gradient Boosting, also called the queen of the ML models is one of the most robust models. Base XGBOOST model (no tuning: Out of Box model) Note:XGBoost works on its own object type, which is Dmatrix. So, datatype conversion is required.\n\n# Create regression matrices, this is requirement for xgboost model\ndtrain_reg = xgb.DMatrix(X_train, y_train, enable_categorical=True)\ndtest_reg =  xgb.DMatrix(X_test, y_test, enable_categorical=True)\n\n\n# use cross validation approach to catch the best boosting round\nn = 1000\n\nmodel_xgb = xgb.cv(\n   dtrain=dtrain_reg,\n   params = {},\n   num_boost_round= n,\n   nfold = 20, #number of folds for cross validation\n   verbose_eval=10, #record rmse every 10 interval\n   early_stopping_rounds = 5,\n   as_pandas = True#stop if there is no improvement in 5 consecutive rounds\n)\n\n[0] train-rmse:79.31111+0.22480 test-rmse:79.31059+4.50293\n[10]    train-rmse:4.32181+0.05969  test-rmse:6.83483+2.17431\n[20]    train-rmse:2.11548+0.06541  test-rmse:6.12915+2.26123\n[30]    train-rmse:1.64633+0.06258  test-rmse:6.04879+2.23034\n[40]    train-rmse:1.29777+0.05879  test-rmse:6.02162+2.23979\n[50]    train-rmse:1.02154+0.05521  test-rmse:5.99733+2.23518\n[54]    train-rmse:0.93376+0.05252  test-rmse:6.00336+2.23321\n\n\n# Extract the optimal number of boosting rounds\noptimal_boosting_rounds = model_xgb['test-rmse-mean'].idxmin()\n\n\n# #using validation sets during training\nevals = [(dtrain_reg, \"train\"), (dtest_reg, \"validation\")]\n\nmodel_xgb = xgb.train(\n   params={},\n   dtrain=dtrain_reg,\n   num_boost_round= optimal_boosting_rounds,\n   evals=evals,#print rmse for every iterations\n   verbose_eval=10, #record rmse every 10 interval\n   early_stopping_rounds = 5 #stop if there is no improvement in 5 consecutive rounds\n)\n\n[0] train-rmse:79.30237 validation-rmse:79.79093\n[10]    train-rmse:4.39108  validation-rmse:4.91840\n[20]    train-rmse:2.09378  validation-rmse:3.83489\n[30]    train-rmse:1.63902  validation-rmse:3.79141\n[40]    train-rmse:1.30465  validation-rmse:3.68415\n[48]    train-rmse:1.08249  validation-rmse:3.61313\n\n# #predict on the the test matrix\npreds = model_xgb.predict(dtest_reg)\n\n#check for rmse\nmse = mean_squared_error(y_test, preds, squared=False)\n\nprint(f\"MSE of the test model: {mse:.3f}\")\n\nMSE of the test model: 3.613\n\n\n**GRID TUNED XGBOOST MODEL\n\n# Define the parameter grid\ngbm_param_grid = {\n    'colsample_bytree': [0.5, 0.7, 0.9],\n    'n_estimators': [100, 200, 300, 1450],\n    'max_depth': [5, 7, 9],\n    'learning_rate': [0.001, 0.01]\n}\n\n#best hyperparameters based on running\ngbm_param_grid_set = {\n    'colsample_bytree': [0.5],\n    'n_estimators': [1450],\n    'max_depth': [5],\n    'learning_rate': [0.01]\n}\n\n# Instantiate the regressor\ngbm = xgb.XGBRegressor()\n\n# Instantiate GridSearchCV with seed\ngridsearch_mse = GridSearchCV(\n    param_grid=gbm_param_grid_set,\n    estimator=gbm,\n    scoring='neg_mean_squared_error',\n    cv=10,\n    verbose=1,\n)\n\n# Fit the gridmse\ngridsearch_mse.fit(X_train, y_train)\n\nGridSearchCV(cv=10,\n             estimator=XGBRegressor(base_score=None, booster=None,\n                                    callbacks=None, colsample_bylevel=None,\n                                    colsample_bynode=None,\n                                    colsample_bytree=None, device=None,\n                                    early_stopping_rounds=None,\n                                    enable_categorical=False, eval_metric=None,\n                                    feature_types=None, gamma=None,\n                                    grow_policy=None, importance_type=None,\n                                    interaction_constraints=None,\n                                    learning_rate=None,...\n                                    max_cat_to_onehot=None, max_delta_step=None,\n                                    max_depth=None, max_leaves=None,\n                                    min_child_weight=None, missing=nan,\n                                    monotone_constraints=None,\n                                    multi_strategy=None, n_estimators=None,\n                                    n_jobs=None, num_parallel_tree=None,\n                                    random_state=None, ...),\n             param_grid={'colsample_bytree': [0.5], 'learning_rate': [0.01],\n                         'max_depth': [5], 'n_estimators': [1450]},\n             scoring='neg_mean_squared_error', verbose=1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  GridSearchCV?Documentation for GridSearchCViFittedGridSearchCV(cv=10,\n             estimator=XGBRegressor(base_score=None, booster=None,\n                                    callbacks=None, colsample_bylevel=None,\n                                    colsample_bynode=None,\n                                    colsample_bytree=None, device=None,\n                                    early_stopping_rounds=None,\n                                    enable_categorical=False, eval_metric=None,\n                                    feature_types=None, gamma=None,\n                                    grow_policy=None, importance_type=None,\n                                    interaction_constraints=None,\n                                    learning_rate=None,...\n                                    max_cat_to_onehot=None, max_delta_step=None,\n                                    max_depth=None, max_leaves=None,\n                                    min_child_weight=None, missing=nan,\n                                    monotone_constraints=None,\n                                    multi_strategy=None, n_estimators=None,\n                                    n_jobs=None, num_parallel_tree=None,\n                                    random_state=None, ...),\n             param_grid={'colsample_bytree': [0.5], 'learning_rate': [0.01],\n                         'max_depth': [5], 'n_estimators': [1450]},\n             scoring='neg_mean_squared_error', verbose=1) best_estimator_: XGBRegressorXGBRegressor(base_score=None, booster=None, callbacks=None,\n             colsample_bylevel=None, colsample_bynode=None,\n             colsample_bytree=0.5, device=None, early_stopping_rounds=None,\n             enable_categorical=False, eval_metric=None, feature_types=None,\n             gamma=None, grow_policy=None, importance_type=None,\n             interaction_constraints=None, learning_rate=0.01, max_bin=None,\n             max_cat_threshold=None, max_cat_to_onehot=None,\n             max_delta_step=None, max_depth=5, max_leaves=None,\n             min_child_weight=None, missing=nan, monotone_constraints=None,\n             multi_strategy=None, n_estimators=1450, n_jobs=None,\n             num_parallel_tree=None, random_state=None, ...) XGBRegressorXGBRegressor(base_score=None, booster=None, callbacks=None,\n             colsample_bylevel=None, colsample_bynode=None,\n             colsample_bytree=0.5, device=None, early_stopping_rounds=None,\n             enable_categorical=False, eval_metric=None, feature_types=None,\n             gamma=None, grow_policy=None, importance_type=None,\n             interaction_constraints=None, learning_rate=0.01, max_bin=None,\n             max_cat_threshold=None, max_cat_to_onehot=None,\n             max_delta_step=None, max_depth=5, max_leaves=None,\n             min_child_weight=None, missing=nan, monotone_constraints=None,\n             multi_strategy=None, n_estimators=1450, n_jobs=None,\n             num_parallel_tree=None, random_state=None, ...) \n\n# Best estimator\nbest_estimator = gridsearch_mse.best_estimator_\n\n# Use the best estimator to make predictions on the test data\ny_pred = best_estimator.predict(X_test)\n\n# Calculate mean squared error\nmse_xgboost = mean_squared_error(y_test, y_pred)\nprint(\"Training Mean Squared Error:\", mse_xgboost)\n\nTraining Mean Squared Error: 8.730223465543306\n\n# Now, use the best estimator to make predictions on the validation data\ny_val_pred = best_estimator.predict(X_val)\n\n# Calculate mean squared error on the validation set\nmse_xgboost_val = mean_squared_error(y_val, y_val_pred)\nprint(\"Validation Mean Squared Error:\", mse_xgboost_val)\n\nValidation Mean Squared Error: 28.505288769482465\n\n# Get the model score on the validation set\nprint(f\"Model score on validation set: {best_estimator.score(X_val, y_val)}\")\n\nModel score on validation set: 0.9978675804165923\n\n\n\nprint(f\"Model score on validation setfor linear regression: {val_r2}\")\n\nModel score on validation setfor linear regression: 0.9964454374974873\n\nprint(f\"Model score on validation set for XGBoost: {best_estimator.score(X_val, y_val)}\")\n\nModel score on validation set for XGBoost: 0.9978675804165923\n\n\nThe XGBoost model has lower Bias, and high accuracy compared to the linear regression model. Thus, I suggest using the XGBOOST model for any new incoming data on ocean values."
  },
  {
    "objectID": "featured_projects/ml_spotify/index.html",
    "href": "featured_projects/ml_spotify/index.html",
    "title": "Comparing competing Machine Learning models in classifying Spotify songs",
    "section": "",
    "text": "In this project, I explored four popular machine learning models (K-Nearest Neighbors, Decision Tree, Bagged Tree, and Random Forest) to classify Spotify songs into two genres: Underground Rap and Drum and Bass (dnb). I used a dataset from the Spotify API, containing 18 audio features of each song. The goal was to determine the best model for this classification task based on accuracy, sensitivity, and specificity.\n\n\nModel Comparison\n\nImplemented and tuned each model using grid search and cross-validation\nEvaluated model performance using accuracy, sensitivity, specificity, and F1 score\nVisualized model performance using confusion matrices and ROC-AUC curves\n\n\n\nResults\n\nRandom Forest model achieved the highest accuracy (92.1%) and F1 score (0.921)\nBagged Tree model showed the highest sensitivity (0.943)\nDecision Tree model had the highest specificity (0.933)\n\n\n#load required libaries\nlibrary(spotifyr) #API interaction\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(rsample)\nlibrary(recipes)\nlibrary(caret)\n\n\nData Preparation\n\n# read the online spotify data\ngenre &lt;- read_csv(\"genres_v2.csv\")\nplaylist &lt;- read_csv(\"playlists.csv\")\n\n\n#jon the data with id from genre and playlist\nspotify_data &lt;- genre %&gt;% \n                left_join(playlist, by =c(\"id\"= \"Playlist\"))\n\n# filter the data to include only the tracks of the two genres you'd like to use for the classification task\nspotify_data &lt;- spotify_data %&gt;% \n                filter(genre %in% c(\"Underground Rap\", \"dnb\")) %&gt;% \n                #rename underground rap to rap\n                mutate(genre = ifelse(genre == \"Underground Rap\", \"URap\", genre))\n\n\n\nData Exploration\n\n# remove columns that you won't be feeding model\nspotify &lt;- spotify_data  %&gt;%  select(song_name, genre, danceability: tempo) %&gt;% \n            #change genre to factor\n            mutate(genre = as.factor(genre))\n\n#find the top 10 most danaceable tracks in the dataset\ntop_20_songs &lt;- spotify %&gt;% \n     arrange(desc(danceability)) %&gt;% \n     head(20) %&gt;% \n     select(song_name, genre, danceability)\n\n# output table with interactivity features\nkableExtra:: kable(top_20_songs,\n                   caption = \"Top 20 most danceable tracks in the dataset\")\n\n\nTop 20 most danceable tracks in the dataset\n\n\nsong_name\ngenre\ndanceability\n\n\n\n\nPOP, LOCK & DROPDEAD\nURap\n0.985\n\n\nLoyaltyRunsDeepInDaLongRun\nURap\n0.985\n\n\nTwo Left Feet Flow\nURap\n0.984\n\n\nHate Your Guts\nURap\n0.983\n\n\nMugen Woe\nURap\n0.982\n\n\nThe 3\nURap\n0.980\n\n\nMavericks\nURap\n0.979\n\n\nTechnicolor\nURap\n0.977\n\n\nKillmonger\nURap\n0.977\n\n\nFunky Friday\nURap\n0.975\n\n\nNervous\nURap\n0.975\n\n\nGo to the Sto\nURap\n0.975\n\n\nBad Bad Bad (feat. Lil Baby)\nURap\n0.974\n\n\nAbraham Lincoln\nURap\n0.974\n\n\nPsycho Pass\nURap\n0.973\n\n\nWho the Fuck Is You\nURap\n0.972\n\n\nDottin Up\nURap\n0.971\n\n\nWorst Day of My Life\nURap\n0.971\n\n\nExcalibur\nURap\n0.970\n\n\nSexy\nURap\n0.970\n\n\n\n\n\n\n#difference between genres for some of the audio features\n#---drop song name, not required for models\nspotify &lt;- spotify %&gt;%  select(-song_name)\n\n#setup manual colors\ncolors &lt;- c(\"#7f58AF\", \"#64C5EB\", \"#E84D8A\", \"#FEB326\", \"lightblue\")\n\n#plot the data\nspotify %&gt;% \n  group_by(genre) %&gt;% \n  summarise(across(danceability:tempo, mean)) %&gt;% \n  pivot_longer(cols = danceability:tempo, names_to = \"audio_feature\", values_to = \"mean\") %&gt;% \n  ggplot(aes(x = genre, y = mean, fill = genre)) +\n  geom_col(position = \"dodge\") +\n  facet_wrap(~audio_feature, scales = \"free\") +\n  theme_classic() +\n  xlab(\"\") +\n  ylab(\"\") +\n  theme(legend.position = \"none\") +\n  scale_fill_manual(values = colors)+\n  #add caption\n  labs(caption = paste0(title = \"Source: Spotify API\")) + \n  #add title at the top center \n  ggtitle(\"Difference between genres for some of the audio features\")\n\n\n\n\n\n\n\n\n\n\n\n1.K-nearest neighbor\nThe k-nearest neighbors (KNN) method is a non-parametric classification algorithm used for recommendation systems. In the context of a Spotify dataset, the KNN algorithm can be employed to recommend songs or artists to a user based on the preferences of other users with similar listening histories. The algorithm calculates the distances between the target user and other users in the dataset based on their features, such as the artists or genres they have listened to. It then selects the k nearest neighbors, which are the users with the shortest distances to the target user, and recommends songs or artists that are popular among these neighbors.\n\n#load the libraries specific for this model\nlibrary(dplyr)    \nlibrary(ggplot2) #great plots\nlibrary(rsample)  #data splitting \nlibrary(recipes) #data preprocessing\nlibrary(skimr) #data exploration\nlibrary(tidymodels) #re-entering tidymodel mode\nlibrary(kknn) #knn modeling\nlibrary(caTools) #for splitting the data\n\n\n#split the data into training and testing set\nset.seed(123)\nsplit = sample.split(spotify$genre, SplitRatio = 0.7)\nspotify_split = initial_split(spotify)\ntrain = subset(spotify, split == TRUE)\ntest =  subset(spotify, split == FALSE)\n\n\n#specify the recipe\nknn_rec &lt;- recipe(genre ~., data = train) %&gt;%\n  step_dummy(all_nominal(), -all_outcomes(), one_hot = T) %&gt;%\n  step_normalize(all_numeric(), -all_outcomes())\n\n## knn spec\nknn_spec &lt;- nearest_neighbor(neighbors = 5) %&gt;%\n  set_engine(\"kknn\") %&gt;%\n  set_mode(\"classification\")\n\n#bake the data\n#baked_train &lt;- bake(knn_rec, train)\n\n\n#apply to testing set\n#baked_test &lt;- bake(knn_rec, test)\n\nTrain the model with 5 folds validation . The model is then tuned with grid search to find the best value of k. The model is then fit to the testing set and the performance is evaluated with confusion matrix, precision, recall and f1 score.\n\n# cross validation on the dataset\ncv_folds &lt;- vfold_cv(train, v = 5)\n\n# put all together in workflow\nknn_workflow &lt;- workflow() %&gt;%\n                add_recipe(knn_rec) %&gt;% \n                add_model(knn_spec)\n                \n\n#fit the resamples and carry out validation\nknn_res &lt;- knn_workflow %&gt;%\n           fit_resamples(resamples = cv_folds, \n           control = control_resamples(save_pred = TRUE))\n\n# find the best value of k\nknn_spec_tune &lt;- nearest_neighbor(neighbors = tune()) %&gt;%\n  set_mode(\"classification\") %&gt;%\n  set_engine(\"kknn\")\n\n# define a new workflow\nwf_knn_tune  &lt;- workflow() %&gt;%\n                add_model(knn_spec_tune) %&gt;% \n                add_recipe(knn_rec)\n  \n# tune the best model with grid search\nfit_knn_cv &lt;-  wf_knn_tune %&gt;%\n  tune_grid(cv_folds, grid = data.frame(neighbors = c(1, 5, seq(10, 100, 10))))\n\n#plot the fit knn cv with changing value of neighbours\nfit_knn_cv %&gt;% collect_metrics() %&gt;% \n  filter(.metric == \"roc_auc\") %&gt;% \n  ggplot(aes(x = neighbors, y = mean)) +\n  geom_point() +\n  geom_line() +\n  labs(title = \"Accuracy of KNN model with different values of k\",\n       x = \"Number of neighbors\",\n       y = \"Accuracy\") +\n  theme_minimal()\n\n\n\n\n\n\n\n# check the performance with collect_metrics\nfit_knn_cv %&gt;% collect_metrics()\n\n# A tibble: 36 × 7\n   neighbors .metric     .estimator   mean     n  std_err .config              \n       &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt; &lt;chr&gt;                \n 1         1 accuracy    binary     0.965      5 0.00170  Preprocessor1_Model01\n 2         1 brier_class binary     0.0351     5 0.00170  Preprocessor1_Model01\n 3         1 roc_auc     binary     0.965      5 0.00309  Preprocessor1_Model01\n 4         5 accuracy    binary     0.968      5 0.00176  Preprocessor1_Model02\n 5         5 brier_class binary     0.0258     5 0.00118  Preprocessor1_Model02\n 6         5 roc_auc     binary     0.989      5 0.00111  Preprocessor1_Model02\n 7        10 accuracy    binary     0.968      5 0.000903 Preprocessor1_Model03\n 8        10 brier_class binary     0.0247     5 0.000804 Preprocessor1_Model03\n 9        10 roc_auc     binary     0.993      5 0.000952 Preprocessor1_Model03\n10        20 accuracy    binary     0.967      5 0.00105  Preprocessor1_Model04\n# ℹ 26 more rows\n\n# create a final workflow\nfinal_wf &lt;- wf_knn_tune %&gt;%\n  finalize_workflow(select_best(fit_knn_cv, metric= \"accuracy\"))\n\n# fit the final model\nfinal_fit &lt;- final_wf %&gt;% fit(data = train)\n\n#predict to testing set\nspotify_pred &lt;- final_fit %&gt;% predict(new_data = test)\n\n# Write over 'final_fit' with this last_fit() approach\nfinal_fit &lt;- final_wf %&gt;% last_fit(spotify_split)\n\n# Collect metrics on the test data!\n\n\nfinal_fit %&gt;% collect_metrics()\n\n# A tibble: 3 × 4\n  .metric     .estimator .estimate .config             \n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy    binary        0.968  Preprocessor1_Model1\n2 roc_auc     binary        0.992  Preprocessor1_Model1\n3 brier_class binary        0.0237 Preprocessor1_Model1\n\n#bind genre from test data to the predicted data\nbind_test_pred &lt;- spotify_pred %&gt;% bind_cols(test)\n\n#plot the confusion matrix\nbind_test_pred %&gt;%\n  conf_mat(truth = genre, estimate = .pred_class) %&gt;% \n  autoplot(type = \"heatmap\") +\n  theme_minimal()+\n  #remove legend\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n#calculate precision, recall and f1 score\nknn_estimate &lt;- bind_test_pred %&gt;% \n                conf_mat(truth = genre, estimate = .pred_class) %&gt;% \n                summary() %&gt;% \n                head(4) %&gt;% \n                #rename .estimate to knn estimate\n                rename(\"knn estimate\" = .estimate)\n\n\n\n2.Decision tree\nDecision trees uses CART algorithm to split the data into two or more homogeneous sets. The algorithm uses the Gini index to create the splits. The model is then tuned with grid search to find the best hyperparameters. The model is then fit to the testing set and the performance is evaluated with confusion matrix, precision, recall and f1 score, as for knn model.\n\n# load the packages for decision trees\nlibrary(MASS)\nlibrary(doParallel)\nlibrary(vip)\n\n# methods using in class\ngenre_split &lt;-  initial_split(spotify)\ngenre_train &lt;-  training(genre_split)\ngenre_test  &lt;-  testing(genre_split)\n\n\n##Preprocess the data\ngenre_rec &lt;- recipe(genre ~., data = genre_train) %&gt;%\n  step_dummy(all_nominal(), -all_outcomes(), one_hot = TRUE) %&gt;%\n  step_normalize(all_numeric(), -all_outcomes())\n\n#new spec, tell the model that we are tuning hypermeter\ntree_spec_tune &lt;- decision_tree(\n  cost_complexity = tune(),\n  tree_depth = tune(),\n  min_n = tune()) %&gt;%\n  set_engine(\"rpart\") %&gt;%\n  set_mode(\"classification\")\n\nUse grid search to find the best hyperparameters and train on folded training set.\n\n# grid search the hyperparameters tuning \ntree_grid &lt;- grid_regular(cost_complexity(), tree_depth(), min_n(), levels = 5)\n\n#set up k-fold cv and use the model on this data\ngenre_cv = genre_train %&gt;% vfold_cv(v = 10)\ndoParallel::registerDoParallel() #build trees in parallel\n\n# setup the grid search\ntree_rs &lt;- tune_grid(tree_spec_tune, genre ~., \n                     resamples = genre_cv,\n                     grid = tree_grid,\n                     metrics = metric_set(accuracy))\n\nUse the workflow to finalize the model and also fit on the training set, and predicting on the test set.\n\n#finalize the model that is cross validate and hyperparameter is tuned\nfinal_tree &lt;- finalize_model(tree_spec_tune, select_best(tree_rs))\n\n#similar functions here.\nfinal_tree_fit &lt;- fit(final_tree, genre~., data = genre_train)\n\n#last_fit() fits on training data (like fit()), but then also evaluates on the test data.\nfinal_tree_result &lt;- last_fit(final_tree, genre~., genre_split)\nfinal_tree_result$.predictions\n\n[[1]]\n# A tibble: 2,211 × 6\n   .pred_dnb .pred_URap  .row .pred_class genre .config             \n       &lt;dbl&gt;      &lt;dbl&gt; &lt;int&gt; &lt;fct&gt;       &lt;fct&gt; &lt;chr&gt;               \n 1   0            1         2 URap        URap  Preprocessor1_Model1\n 2   0            1         3 URap        URap  Preprocessor1_Model1\n 3   0            1         4 URap        URap  Preprocessor1_Model1\n 4   0.00157      0.998     6 URap        URap  Preprocessor1_Model1\n 5   0            1         7 URap        URap  Preprocessor1_Model1\n 6   0.00157      0.998     8 URap        URap  Preprocessor1_Model1\n 7   0            1        14 URap        URap  Preprocessor1_Model1\n 8   0.00157      0.998    16 URap        URap  Preprocessor1_Model1\n 9   0.00157      0.998    17 URap        URap  Preprocessor1_Model1\n10   0.00157      0.998    20 URap        URap  Preprocessor1_Model1\n# ℹ 2,201 more rows\n\n\n\n#visualize the model\nfinal_tree_fit %&gt;%\n  vip(geom = \"col\", aesthetics = list(fill = \"midnightblue\", alpha = 0.8)) +\n  scale_y_continuous(expand = c(0,0))\n\n\n\n\n\n\n\n# how accurate is this model on the test data?\nfinal_tree_result %&gt;% collect_metrics()\n\n# A tibble: 3 × 4\n  .metric     .estimator .estimate .config             \n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy    binary        0.983  Preprocessor1_Model1\n2 roc_auc     binary        0.982  Preprocessor1_Model1\n3 brier_class binary        0.0166 Preprocessor1_Model1\n\n#plot the confusion matrix\nfinal_tree_result %&gt;% \n  collect_predictions() %&gt;% \n  conf_mat(truth = genre, estimate = .pred_class) %&gt;% \n  autoplot(type = \"heatmap\") +\n  theme_minimal()+\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n#plot precision, recall and f1 score\ndt_estimate &lt;- final_tree_result %&gt;% \n                collect_predictions() %&gt;% \n                conf_mat(truth = genre, estimate = .pred_class) %&gt;% \n                summary() %&gt;% \n                head(4) %&gt;% \n                #rename .estimate to deicsion tree estimate\n                rename(\"decision tree estimate\" = .estimate)\n\n\n\n3.Bagged tree\nBagging is a method of ensemble learning that combines the predictions of multiple models to improve the overall performance. It is different from two previous model in the sense that it is collection of ML models. It works by training multiple models on different subsets of the training data and then combining their predictions to make a final prediction. The bagged tree uses decision trees as the base model. The model is then tuned with grid search to find the best hyperparameters. The model is then fit to the testing set and the performance is evaluated with confusion matrix, precision, recall and f1 score, as for knn model and the decision tree model.\n\n# Helper packages\nlibrary(doParallel)  # for parallel backend to foreach\nlibrary(foreach)     # for parallel processing with for loops\nlibrary(caret)       # for general model fitting\nlibrary(rpart)       # for fitting decision trees\nlibrary(ipred)       # for bagging\nlibrary(baguette)    # for bagging\nlibrary(tidymodels)  # Assuming you have tidymodels installed\n\n# Methods using in class\ngenre_split &lt;- initial_split(spotify)\ngenre_train &lt;- training(genre_split)\ngenre_test  &lt;- testing(genre_split)\n\n## Preprocess the data\ngenre_rec &lt;- recipe(genre ~ ., data = genre_train) %&gt;%\n         step_dummy(all_nominal(), -all_outcomes(), one_hot = TRUE) %&gt;%\n         step_normalize(all_numeric(), -all_outcomes(),)\n\n# instatntiate bag model\nbag_model &lt;- bag_tree() %&gt;%\n     set_engine(\"rpart\", times = 100) %&gt;%\n     set_mode(\"classification\")\n\n# create a workflow\nbag_workflow &lt;- workflow() %&gt;%\n    add_model(bag_model) %&gt;%\n    add_recipe(genre_rec)\n\n\n##folds the data for validation set\ngenre_cv = genre_train %&gt;% vfold_cv(v = 10)\n\n# use tune grid to tune the model\nbag_tune &lt;- tune_grid(bag_workflow,\n                      resamples = genre_cv,\n                      grid = 10)\n\n# collect metrices\nbag_tune %&gt;% collect_metrics()\n\n# A tibble: 3 × 6\n  .metric     .estimator    mean     n  std_err .config             \n  &lt;chr&gt;       &lt;chr&gt;        &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy    binary     0.990      10 0.00156  Preprocessor1_Model1\n2 brier_class binary     0.00833    10 0.00112  Preprocessor1_Model1\n3 roc_auc     binary     0.998      10 0.000354 Preprocessor1_Model1\n\n\n\n#finalize the workflow\nbag_best = show_best(bag_tune, n = 1,  metric = \"roc_auc\")\n\n#fit the model\nfinalize_bag &lt;- finalize_workflow(bag_workflow, select_best(bag_tune, metric = \"roc_auc\" ))\n\n# fit the finalized model \nbag_fit &lt;- finalize_bag %&gt;% fit(genre_train)\n\n# predict the model on test\nbag_pred &lt;- bag_fit %&gt;% predict(genre_test) %&gt;% \n            bind_cols(genre_test)\n\n#predict the model with probaility values\nbag_pred_prob &lt;- bag_fit %&gt;% predict(genre_test, type = \"prob\") %&gt;% \n            bind_cols(genre_test)\n\n#model metrics and evaluation\naccuracy(bag_pred, truth = genre, estimate = .pred_class)\n\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.992\n\n#visualize the model\nbag_pred %&gt;%\n  conf_mat(truth = genre, estimate = .pred_class) %&gt;% \n  autoplot(type = \"heatmap\") +\n  theme_minimal()\n\n\n\n\n\n\n\n  #remove legend\n\n\ntheme(legend.position = \"none\")\n\nList of 1\n $ legend.position: chr \"none\"\n - attr(*, \"class\")= chr [1:2] \"theme\" \"gg\"\n - attr(*, \"complete\")= logi FALSE\n - attr(*, \"validate\")= logi TRUE\n\n#plot precision, recall and f1 score\nbag_estimate &lt;- bag_pred %&gt;% \n                conf_mat(truth = genre, estimate = .pred_class) %&gt;% \n                summary() %&gt;% \n                head(4) %&gt;% \n                #rename .estimate to bagged tree estimate\n                rename(\"bagged tree estimate\" = .estimate)\n\n\n\n4.Random Forest\nRandom forest is a popular ensemble learning method that combines the predictions of multiple decision trees to improve the overall performance. It is different from bagged tree in the sense that it uses a collection of decision trees as the base model, stochastically chosing number of columns too in training. That is not seen in general bagging model. The model is then tuned with grid search to find the best hyperparameters.\n\n# random forest with R\nlibrary(ranger) # for random forest\n\n# methods using in class\ngenre_split &lt;- initial_split(spotify)\ngenre_train &lt;- training(genre_split)\ngenre_test &lt;-  testing(genre_split)\n\n# fold the data for validation set\ncv_folds = vfold_cv(genre_train, v = 5)\n\n# create a previous recipe\ngenre_recipe &lt;- recipe(genre ~., data = genre_train) %&gt;%\n          step_dummy(all_nominal(), -all_outcomes(), one_hot = TRUE) %&gt;%\n          step_normalize(all_numeric(), -all_outcomes())\n\n# instantiate model\nrf_model &lt;- rand_forest(mtry = tune(), trees = tune()) %&gt;%\n         set_engine(\"ranger\") %&gt;%\n         set_mode(\"classification\")\n\n# create a workflow\nrf_workflow &lt;- workflow() %&gt;%\n         add_model(rf_model) %&gt;%\n         add_recipe(genre_recipe)\n\n# use tune grid to tune the model\nrf_tune &lt;- tune_grid(\n        rf_workflow,\n        resamples = cv_folds,\n        grid = 10)\n\n# collect metrices\nrf_tune %&gt;% collect_metrics()\n\n# A tibble: 30 × 8\n    mtry trees .metric     .estimator    mean     n   std_err .config           \n   &lt;int&gt; &lt;int&gt; &lt;chr&gt;       &lt;chr&gt;        &lt;dbl&gt; &lt;int&gt;     &lt;dbl&gt; &lt;chr&gt;             \n 1    10  1748 accuracy    binary     0.991       5 0.00108   Preprocessor1_Mod…\n 2    10  1748 brier_class binary     0.00757     5 0.000706  Preprocessor1_Mod…\n 3    10  1748 roc_auc     binary     0.999       5 0.000199  Preprocessor1_Mod…\n 4     7    69 accuracy    binary     0.992       5 0.000983  Preprocessor1_Mod…\n 5     7    69 brier_class binary     0.00661     5 0.000573  Preprocessor1_Mod…\n 6     7    69 roc_auc     binary     1.00        5 0.000158  Preprocessor1_Mod…\n 7     5  1242 accuracy    binary     0.993       5 0.000805  Preprocessor1_Mod…\n 8     5  1242 brier_class binary     0.00665     5 0.000585  Preprocessor1_Mod…\n 9     5  1242 roc_auc     binary     1.00        5 0.0000814 Preprocessor1_Mod…\n10     6   464 accuracy    binary     0.993       5 0.000873  Preprocessor1_Mod…\n# ℹ 20 more rows\n\n\n\n#finalize workflow\nrf_best = show_best(rf_tune, n = 1,  metric = \"roc_auc\")\n\n# finalize the model \nfinal_rand_forest &lt;- finalize_workflow(rf_workflow, select_best(rf_tune, metric = \"roc_auc\" ))\n\n#use this to predict the on test set\nfinal_rf_fit &lt;- final_rand_forest %&gt;% fit(genre_train)\n  \n# predict the model on test\nfinal_rf_pred &lt;- final_rf_fit %&gt;% predict(genre_test) %&gt;% \n            bind_cols(genre_test)\n\n#model metrics and evaluation\naccuracy(final_rf_pred, truth = genre, estimate = .pred_class)\n\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.993\n\n#visualize the model\nfinal_rf_pred %&gt;%\n  conf_mat(truth = genre, estimate = .pred_class) %&gt;% \n  autoplot(type = \"heatmap\") +\n  theme_minimal()+\n  #remove legend\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n#plot precision, recall and f1 score\nrf_estimate &lt;- final_rf_pred %&gt;% \n                conf_mat(truth = genre, estimate = .pred_class) %&gt;% \n                summary() %&gt;% \n                head(4) %&gt;% \n                #rename .estimate to random forest estimate\n                rename(\"random forest estimate\" = .estimate)\n\n\n\nModel Comparison\n\n#Compare the accuracy of all models and create a dataframe\n\n#bind all the estimates using left join\nestimate_table &lt;- knn_estimate %&gt;% \n                left_join(dt_estimate) %&gt;% \n                left_join(bag_estimate) %&gt;% \n                left_join(rf_estimate)\n\n\nestimate_table\n\n# A tibble: 4 × 6\n  .metric  .estimator `knn estimate` `decision tree estimate`\n  &lt;chr&gt;    &lt;chr&gt;               &lt;dbl&gt;                    &lt;dbl&gt;\n1 accuracy binary              0.970                    0.983\n2 kap      binary              0.933                    0.963\n3 sens     binary              0.970                    0.976\n4 spec     binary              0.970                    0.987\n# ℹ 2 more variables: `bagged tree estimate` &lt;dbl&gt;,\n#   `random forest estimate` &lt;dbl&gt;\n\n\n\n## plot the histogram of the accuracy of all models\nestimate_table %&gt;% \n  pivot_longer(cols = c(\"knn estimate\", \"decision tree estimate\", \"bagged tree estimate\", \"random forest estimate\"), names_to = \"model\", values_to = \"accuracy\") %&gt;% \n  ggplot(aes(x = model, y = accuracy, fill = model)) +\n  geom_col(position = \"dodge\") +\n  theme_minimal() +\n  xlab(\"\") +\n  ylab(\"accuracy\") +\n  theme(legend.position = \"none\") +\n  labs(caption = paste0(title = \"Source: Spotify API\")) + \n  ggtitle(\"Accuracy of all models\") + \n  #transform y axis to percentage multiplying by 100\n  scale_y_continuous(labels = scales::percent)+\n  #fill manual colors\n  scale_fill_manual(values = colors)\n\n\n\n\n\n\n\n\n\n\nConclusion\nThis project demonstrated the effectiveness of ensemble learning methods (Bagged Tree and Random Forest) in classifying Spotify songs into two genres. The results can be used to inform music recommendation systems or genre classification tasks. I showcased my skills in data preprocessing, model implementation, hyperparameter tuning, and performance evaluation."
  },
  {
    "objectID": "featured_projects/marketing/index.html",
    "href": "featured_projects/marketing/index.html",
    "title": "Exploring the Impact of Marketing Strategiese: A Deep Dive into EDA and A/B Testing",
    "section": "",
    "text": "In the dynamic world of marketing, understanding customer behavior and optimizing strategies is paramount for success. As data scientists, we play a crucial role in extracting actionable insights from vast datasets. In this blog post, we will explore how exploratory data analysis (EDA) and A/B testing can be employed to enhance marketing campaigns, using a real-world marketing dataset. #### Understanding the Dataset Our dataset, collected from a marketing campaign, comprises various attributes, including user demographics, marketing channels, and conversion outcomes. The initial steps in our analysis involve loading and preprocessing the data to ensure its integrity and suitability for analysis\n\nExploring the Impact of Marketing Strategies Through Data Science: A Deep Dive into EDA and A/B Testing\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom scipy import stats  # for AB testing\n\n# Load the dataset\nmarketing = pd.read_csv(\"data/marketing.csv\")\nprint(marketing.head(5))\n\n      user_id date_served  ... subscribing_channel is_retained\n0  a100000029      1/1/18  ...           House Ads        True\n1  a100000030      1/1/18  ...           House Ads        True\n2  a100000031      1/1/18  ...           House Ads        True\n3  a100000032      1/1/18  ...           House Ads        True\n4  a100000033      1/1/18  ...           House Ads        True\n\n[5 rows x 12 columns]\n\n\nTo facilitate our analysis, we map categorical marketing channels to numeric values. This transformation enables more efficient computations and visualizations.\n\n# Mapping channels to numeric values\nchannel_dict = {\n    \"House Ads\": 1, \"Instagram\": 2,\n    \"Facebook\": 3, \"Email\": 4, \"Push\": 5\n}\n\n# Map the channel to channel code\nmarketing['channel_code'] = marketing['subscribing_channel'].map(channel_dict)\n\n\nPerforming Exploratory Data Analysis (EDA)\nEDA is a critical step in understanding our data. By calculating key marketing metrics such as total users, retention rates, and conversion rates, we can assess the effectiveness of different marketing channels.\nTo automate the EDA process, we create a function to calculate conversion rates based on user segments. This function allows us to quickly assess the performance of different marketing strategies.\n\ndef conversion_rate(dataframe, column_names):\n    #total number of converted users\n    column_conv = dataframe[dataframe['converted']== True].groupby(column_names)['user_id'].nunique()\n    #total number of users\n    column_total= dataframe.groupby(column_names)['user_id'].nunique()\n    #conversion rate\n    conversion_rate = column_conv/column_total\n    #fill missing values with zero\n    conversion_rate = conversion_rate.fillna(0)\n    return conversion_rate\n\nUsing this function, we can calculate conversion rates by age group and visualize the results to identify trends in user engagement.\n\n# calculate conversion rate by age group\nage_group_conv = conversion_rate(marketing, ['date_served', 'age_group'])\n\n#unstack and create a dataframe\nage_group_df = pd.DataFrame(age_group_conv.unstack(level = 1))\n\n#visualize conversion by age group\nage_group_df.plot()\n# a function that should produce plot based on grouping based on one column\ndef plotting_conv(dataframe):\n    for column in dataframe:\n        # Plot column by dataframe's index\n        plt.plot(dataframe.index, dataframe[column])\n        plt.title('Daily ' + str(column) + ' Conversion Rate', size=16)\n        plt.xlabel('Date', size=14)\n        \n        # Set x-axis labels to vertical\n        plt.xticks(rotation=90)\n\n        # Show plot\n        plt.show()\n        plt.clf()\n\n#calculate conversion rate by dateserved and age group\nage_group_conv = conversion_rate(marketing, ['date_served', 'age_group'])\n\n#unstakc age_group_conv and craete a dataframe\nage_group_df = pd.DataFrame(age_group_conv.unstack(level=1))\n\n#plot the results\nplotting_conv(age_group_df)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnalyzing the Impact of the Day of the Week\nUnderstanding temporal patterns in marketing effectiveness can provide valuable insights. We examine whether the day of the week affects conversion rates, especially for House Ads, which often see varying performance based on timing.\n\n#calculate conversion rate by date_served and channel\ndaily_conv_channel = conversion_rate(marketing, ['date_served', 'marketing_channel'])\nprint(daily_conv_channel)\n\ndate_served  marketing_channel\n1/1/18       Email                1.000000\n             Facebook             0.117647\n             House Ads            0.084656\n             Instagram            0.106667\n             Push                 0.083333\n                                    ...   \n1/9/18       Email                0.500000\n             Facebook             0.120690\n             House Ads            0.127389\n             Instagram            0.152542\n             Push                 0.054054\nName: user_id, Length: 155, dtype: float64\n\n#add day of week column to marketing\nmarketing['date_served'] = pd.to_datetime(marketing['date_served'])\n\n&lt;string&gt;:2: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n\nmarketing['DoW_served'] = marketing['date_served'].dt.dayofweek\n\n#calculate conversion rate by day of week\nDoW_conversion = conversion_rate(marketing, ['DoW_served', 'marketing_channel'])\n\n#unstack channels\nDoW_df = pd.DataFrame(DoW_conversion.unstack(level = 1))\n\n#plot conversion rate by day of week\nDoW_df.plot()\nplt.title('Conversion rate by day of week')\nplt.ylim(0)\n\n(0.0, 0.9487847222222222)\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\nSegmenting Users for Deeper Insights\nTo better understand how language preferences impact conversion, we segment our data by the displayed language and examine conversion rates.\n\n#Isolate hte rows where marking channel is house ads\nhouse_ads = marketing[marketing['marketing_channel']== 'House Ads']\n\n#calculate the conversion by date served and language displayed\nconv_lang_channel = conversion_rate(house_ads, ['date_served', 'language_displayed'])\n\n#unstack conv_lang_channel\nconv_lang_df = pd.DataFrame(conv_lang_channel.unstack(level=1))\n\n#use plotting fucntions to display results\nplotting_conv(conv_lang_df)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can also assess whether users received messages in their preferred language, revealing further insights into potential barriers to conversion.\n\n# Add the new column for language correctness\nhouse_ads['is_correct_lang'] = np.where(house_ads['language_preferred'] == house_ads['language_displayed'], 'Yes', 'No')\n\n&lt;string&gt;:2: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n# Group by date served and language correctness\nlanguage_check = house_ads.groupby(['date_served', 'is_correct_lang'])['is_correct_lang'].count()\nlanguage_check_df = pd.DataFrame(language_check.unstack(level=1)).fillna(0)\n\n# Print results and calculate percentage of correct language\nlanguage_check_df['pct'] = language_check_df['Yes'] / language_check_df.sum(axis=1)\n\n# Plot results\nplt.plot(language_check_df.index.values, language_check_df['pct'])\nplt.title('Correct Language Display Rate Over Time')\nplt.xlabel('Date')\n# Set x-axis labels to vertical\nplt.xticks(rotation=90)\n\n(array([17532., 17536., 17540., 17544., 17548., 17552., 17556., 17560.,\n       17563.]), [Text(17532.0, 0, '2018-01-01'), Text(17536.0, 0, '2018-01-05'), Text(17540.0, 0, '2018-01-09'), Text(17544.0, 0, '2018-01-13'), Text(17548.0, 0, '2018-01-17'), Text(17552.0, 0, '2018-01-21'), Text(17556.0, 0, '2018-01-25'), Text(17560.0, 0, '2018-01-29'), Text(17563.0, 0, '2018-02-01')])\n\nplt.ylabel('Percentage')\nplt.show()\n\n\n\n\n\n\n\n\n\n#calcualte pre_error conversion rate\nhouse_ads_bug = house_ads[house_ads['date_served'] &lt; '2018-01-11']\nlang_conv = conversion_rate(house_ads_bug, ['language_displayed'])\n\n#Index other language conversion rate against English\nspanish_index = lang_conv['Spanish']/lang_conv['English']\narabic_index = lang_conv['Arabic']/lang_conv['English']\ngerman_index = lang_conv['German']/lang_conv['English']\n\nprint(\"Spanish index:\", spanish_index)\n\nSpanish index: 1.681924882629108\n\nprint(\"Arabic index:\", arabic_index)\n\nArabic index: 5.045774647887324\n\nprint(\"German index:\", german_index)\n\nGerman index: 4.485133020344287\n\n\n\n#Group house_ads by date and language\nconverted = house_ads.groupby(['date_served', 'language_preferred']).agg({'user_id': 'nunique', 'converted': 'sum'})\n\n#unstacked convereted\nconverted_df = pd.DataFrame(converted.unstack(level = 1))\n# Create English conversion rate column for affected period\nconverted_df['english_conv_rate'] = converted_df.loc['2018-01-11':'2018-01-31'][('converted','English')]/converted_df.loc['2018-01-11':'2018-01-31'][('user_id','English')]\n\n# Create expected conversion rates for each language\nconverted_df['expected_spanish_rate'] = converted_df['english_conv_rate']*spanish_index\nconverted_df['expected_arabic_rate'] = converted_df['english_conv_rate']*arabic_index\nconverted_df['expected_german_rate'] = converted_df['english_conv_rate']*german_index\n\n# Multiply number of users by the expected conversion rate\nconverted_df['expected_spanish_conv'] = converted_df['expected_spanish_rate']/100*converted_df[('user_id','Spanish')]\nconverted_df['expected_arabic_conv'] = converted_df['expected_arabic_rate']/100*converted_df[('user_id','Arabic')]\nconverted_df['expected_german_conv'] = converted_df['expected_german_rate']/100*converted_df[('user_id','German')]\n\n\n# use .loc to slice only the relevant dates\nconverted = converted_df.loc['2018-01-11':'2018-01-31']\n\n#sum expect subscribers for each language\nexpected_subs = converted['expected_spanish_conv'].sum() + converted['expected_arabic_conv'].sum() + converted['expected_german_conv'].sum()\n\n#calculate how many subscribers we actually got\nactual_subs = converted[('converted', 'Spanish')].sum()  + converted[('converted', 'Arabic')].sum() + converted[('converted', 'German')].sum()\n#substract how many subscribers we got despite the bug\nlost_subs = expected_subs - actual_subs\nprint(lost_subs)\n\n-25.495425075261792\n\n\n\n\nA/B Testing: Evaluating Email Campaigns\nTo assess the effectiveness of our marketing strategies, we conduct an A/B test on our email campaigns. By segmenting our users into control and personalization groups, we can evaluate which approach yields better conversion rates.\n\n# Subset the DataFrame for email marketing\nemail = marketing[marketing['marketing_channel'] == 'Email']\n\n# Group the email DataFrame by variant\nalloc = email.groupby(['variant'])['user_id'].nunique()\n\n# Plot test allocation\nalloc.plot(kind='bar')\nplt.title('Email Personalization Test Allocation')\nplt.ylabel('# Participants')\nplt.show()\n\n\n\n\n\n\n\n\n\n# Group marketing by user_id and variant\nsubscribers = email.groupby(['user_id', 'variant'])['converted'].max()\nsubscribers_df = pd.DataFrame(subscribers.unstack(level=1))\n\n# Calculate conversion rates\ncontrol = subscribers_df['control'].dropna()\npersonalization = subscribers_df['personalization'].dropna()\n\nprint('Control conversion rate:', np.mean(control))\n\nControl conversion rate: 0.2814814814814815\n\nprint('Personalization conversion rate:', np.mean(personalization))\n\nPersonalization conversion rate: 0.3908450704225352\n\n\nTo evaluate the effectiveness of personalization, we define a function to calculate the lift in conversion rates between the two groups.\n\ndef lift(a,b):\n    # Calcuate the mean of a and b \n    a_mean = np.mean(a)\n    b_mean = np.mean(b)\n    \n    # Calculate the lift using a_mean and b_mean\n    lift = (b_mean-a_mean)/a_mean\n  \n    return str(round(lift*100, 2)) + '%'\n  \n# Print lift() with control and personalization as inputs\nprint(lift(control, personalization))\n\n38.85%\n\n\n\n\nSegmenting for Targeted Insights in A/B Testing\nFinally, we can perform segmented A/B testing based on various user demographics, such as language displayed and age group, allowing us to uncover nuanced insights into user behavior.\n\ndef ab_segmentation(segment):\n    for subsegment in np.unique(marketing[segment].values):\n        print(f'Segment: {subsegment}')\n        \n        email = marketing[(marketing['marketing_channel'] == 'Email') & (marketing[segment] == subsegment)]\n        email.loc[:, 'converted'] = pd.to_numeric(email['converted'], errors='coerce')\n        \n        subscribers = email.groupby(['user_id', 'variant'])['converted'].max()\n        subscribers = pd.DataFrame(subscribers.unstack(level=1))\n        \n        control = pd.to_numeric(subscribers['control'], errors='coerce').dropna()\n        personalization = pd.to_numeric(subscribers['personalization'], errors='coerce').dropna()\n\n        if len(control) &gt; 0 and len(personalization) &gt; 0:\n            print('Lift:', lift(control, personalization))\n            print('T-statistic:', stats.ttest_ind(control, personalization), '\\n\\n')\n        else:\n            print('Not enough data to perform t-test for', subsegment)\n\n# Segmenting based on language displayed and age group\nab_segmentation('language_displayed')\n\nSegment: Arabic\nLift: 50.0%\nT-statistic: TtestResult(statistic=np.float64(-0.5773502691896255), pvalue=np.float64(0.5795840000000001), df=np.float64(8.0)) \n\n\nSegment: English\nLift: 39.0%\nT-statistic: TtestResult(statistic=np.float64(-2.2183598646203215), pvalue=np.float64(0.026991701290720503), df=np.float64(486.0)) \n\n\nSegment: German\nLift: -1.62%\nT-statistic: TtestResult(statistic=np.float64(0.19100834180787182), pvalue=np.float64(0.8494394170062678), df=np.float64(42.0)) \n\n\nSegment: Spanish\nLift: 166.67%\n/Users/sujanbhattarai/.virtualenvs/r-reticulate/lib/python3.9/site-packages/scipy/stats/_axis_nan_policy.py:531: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n  res = hypotest_fun_out(*samples, **kwds)\nT-statistic: TtestResult(statistic=np.float64(-2.3570226039551585), pvalue=np.float64(0.04015671811047753), df=np.float64(10.0)) \n\nab_segmentation('age_group')\n\nSegment: 0-18 years\nLift: 121.4%\nT-statistic: TtestResult(statistic=np.float64(-2.966044912142212), pvalue=np.float64(0.003872449439129706), df=np.float64(89.0)) \n\n\nSegment: 19-24 years\nLift: 106.24%\nT-statistic: TtestResult(statistic=np.float64(-3.0317943847866697), pvalue=np.float64(0.0030623836114689195), df=np.float64(105.0)) \n\n\nSegment: 24-30 years\nLift: 161.19%\nT-statistic: TtestResult(statistic=np.float64(-3.861539544326876), pvalue=np.float64(0.00018743381094867337), df=np.float64(114.0)) \n\n\nSegment: 30-36 years\nLift: -100.0%\nT-statistic: TtestResult(statistic=np.float64(3.185906464414798), pvalue=np.float64(0.002323848743176535), df=np.float64(58.0)) \n\n\nSegment: 36-45 years\nLift: -85.23%\nT-statistic: TtestResult(statistic=np.float64(2.431790127931851), pvalue=np.float64(0.017975686009788244), df=np.float64(61.0)) \n\n\nSegment: 45-55 years\nLift: -72.22%\nT-statistic: TtestResult(statistic=np.float64(2.0654991273179326), pvalue=np.float64(0.04306233968820124), df=np.float64(62.0)) \n\n\nSegment: 55+ years\nLift: -100.0%\nT-statistic: TtestResult(statistic=np.float64(3.326565456420339), pvalue=np.float64(0.0016358623456360468), df=np.float64(51.0)) \n\n\n\n\nConclusion\nThrough EDA and A/B testing, data scientists can derive meaningful insights from marketing datasets, informing strategic decisions and optimizing marketing campaigns. By leveraging data analysis, we empower organizations to better understand their audiences and enhance their marketing efforts. This comprehensive approach not only drives engagement but also boosts conversion rates, ultimately leading to a more effective marketing strategy.\nIn an era where data is king, the role of the data scientist is not just to analyze data but to transform insights into actionable strategies that propel businesses forward."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "",
    "section": "",
    "text": "Areas of Proficiency\n  \n\n\n\n  \n     SUJAN BHATTARAI\n     Welcome to The Dataverse\n  \n  \n    \n      Areas of Proficiency\n    \n    \n      \n        Data Science\n        Enjoys analytical thinking, data-driven decisions, and content creation about Data Science\n      \n      \n        Machine Learning\n        Strong understanding of maths in ML algorithms: neural networks, tree based models, regressions\n      \n      \n        Causal Inference\n        Experience with First order effects, DID estimate, panel data, inverse weighted regression\n      \n      \n        Databases\n        Proficient at RDBMS, experience with snowflake and big data, concepts on HDFS and Hadoop \n      \n      \n        Modelling\n         Ran modelling simulations, randomize experimental designs, measure parameters uncertainties\n      \n      \n        Data Visualization\n         Knowledge of data visualization concepts, expert at ggplot and ggplot-children packages"
  },
  {
    "objectID": "certifications.html",
    "href": "certifications.html",
    "title": "",
    "section": "",
    "text": "Certificate Page\n    \n\n\n    \n    \n\n    \n    \n        \n            Data Science with R\n            \n                Fundamentals of R programming\n                Data manipulation with dplyr and tidyr\n                Data visualization using ggplot2\n                Statistical analysis and hypothesis testing\n                Machine learning with R\n            \n        \n        \n            \n        \n    \n\n    \n        \n            SAS programming\n            \n                SAS function, tables, Schemas\n                Data cleaning and analysis with SAS\n                Statistical analysis with SAS procedures\n                Creation of reports and graphics\n            \n        \n        \n            \n        \n    \n\n    \n        \n            Concepts on Big data\n            \n                Conceptual understanding of Big Data\n                Big Data technologies and ecosystems\n                Data processing and storage techniques\n                Big Data analytics and applications\n            \n        \n        \n            \n        \n    \n\n    \n        \n            Big Data Modelling\n            \n                Understanding of Big Data modeling techniques\n                Application of predictive analytics in Big Data\n                Hands-on experience with Hadoop\n                Designing and implementing Big Data solutions\n            \n        \n        \n            \n        \n    \n\n    \n        \n            Data Analyst with SQL\n            \n                SQL queries and RDBMS foundation\n                Data manipulation with SQL\n                Joining and aggregating data\n                Writing subqueries and CTEs\n            \n        \n        \n            \n        \n    \n\n    \n    \n     \n        \n            Hands-On Essentials: Data Warehouse\n            \n                Create, edit, and load snoflake Tables\n                Create and Use Snowflake resources\n                Create and edit COPY statements\n                Transform and parse CSV and JSON data"
  }
]